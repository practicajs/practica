"use strict";(self.webpackChunkpractica_docs=self.webpackChunkpractica_docs||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"a-compilation-of-outstanding-testing-articles-with-javaScript","metadata":{"permalink":"/blog/a-compilation-of-outstanding-testing-articles-with-javaScript","editUrl":"https://github.com/practicajs/practica/tree/main/docs/blog/10-masterpiece-articles/index.md","source":"@site/blog/10-masterpiece-articles/index.md","title":"A compilation of outstanding testing articles (with JavaScript)","description":"What\'s special about this article?","date":"2023-08-06T10:00:00.000Z","formattedDate":"August 6, 2023","tags":[{"label":"node.js","permalink":"/blog/tags/node-js"},{"label":"testing","permalink":"/blog/tags/testing"},{"label":"javascript","permalink":"/blog/tags/javascript"},{"label":"tdd","permalink":"/blog/tags/tdd"},{"label":"unit","permalink":"/blog/tags/unit"},{"label":"integration","permalink":"/blog/tags/integration"}],"readingTime":12.025,"hasTruncateMarker":false,"authors":[{"name":"Yoni Goldberg","title":"Practica.js core maintainer","url":"https://github.com/goldbergyoni","imageURL":"https://github.com/goldbergyoni.png","key":"goldbergyoni"}],"frontMatter":{"slug":"a-compilation-of-outstanding-testing-articles-with-javaScript","date":"2023-08-06T10:00","hide_table_of_contents":true,"title":"A compilation of outstanding testing articles (with JavaScript)","authors":["goldbergyoni"],"tags":["node.js","testing","javascript","tdd","unit","integration"]},"nextItem":{"title":"Testing the dark scenarios of your Node.js application","permalink":"/blog/testing-the-dark-scenarios-of-your-nodejs-application"}},"content":"## What\'s special about this article?\\n\\nAs a testing consultant, I read tons of testing articles throughout the years. The majority is nice-to-read, casual pieces of content which not always worth your precious time. Once in a while, not very often, I landed on an article that was _shockingly good_ and could genuinely improve your test writing skills. I\'ve cherry-picked these outstanding articles for you, and added my abstract nearby. Half of these articles are related directly to JavaScript/Node.js, the second half covers ubiquitous testing concepts that are applicable in every language\\n\\nWhy did I find these articles to be outstanding? First, the writing quality is excellent. Second, they deal with the \'new world of testing\', not the commonly known \'TDD-ish\' stuff but rather modern concepts and tooling\\n\\nToo busy to read them all? Search for articles that are decorated with a medal \ud83c\udfc5, these are a true masterpiece pieces of content that you never wanna miss\\n\\n**Before we start:** If you haven\'t heard, I launched my comprehensive Node.js testing course a week ago  ([curriculum here](https://testjavascript.com/curriculum2/)). There are less than 48 hours left for the [\ud83c\udf81 special launch deal](https://courses.testjavascript.com/p/node-js-javascript-testing-from-a-to-z)\\n\\nHere they are, 10 outstanding testing articles:\\n\\n<br/>\\n\\n## \ud83d\udcc4 1. \'Selective Unit Testing \u2013 Costs and Benefits\'\\n\\n**\u270d\ufe0f Author:** Steve Sanderson\\n\\n**\ud83d\udd16 Abstract:** We all found ourselves at least once in the ongoing and flammable discussion about \'units\' vs \'integration\'. This articles delve into a greater level of specificity and discuss WHEN unit test shine by considering the costs of writing these tests under *various scenarios*. Many treat their testing strategy as a static model - a testing technique they always write regardless of the context. \\"Always write unit tests against functions\\", \\"Write mostly integration tests\\" are a type of arguments often heard. Conversely, this article suggests that the attractiveness of unit tests should be evaluated based on the *costs and benefits per module*. The article classifies multiple scenarios where the net value of unit tests is high or low, for example:\\n\\n> If your code is basically obvious \u2013 so at a glance you can see exactly what it does \u2013 then additional design and verification (e.g., through unit testing) yields extremely minimal benefit, if any\\n\\nThe author also puts a 2x2 model to visualize when the attractiveness of unit tests is high or low\\n\\n![When unit shines](./selective-unit-tests.png)\\n\\nSide note, not part of the article: Personally I (Yoni) always start with component tests, outside-in, cover first the high-level user flow details (a.k.a [the testing diamond](https://www.crispy-engineering.com/p/why-test-diamond-model-makes-sense)). Then later once I have functions, I add unit tests based on their net value. This article helped me a lot in classifying and evaluating the benefits of units in various scenarios\\n\\n\\n**\ud83d\udc53 Read time:** 9 min (1850 words)\\n\\n**\ud83d\udd17 Link:** [https://blog.stevensanderson.com/2009/11/04/selective-unit-testing-costs-and-benefits/](https://blog.stevensanderson.com/2009/11/04/selective-unit-testing-costs-and-benefits/)\\n\\n<br/>\\n\\n## \ud83d\udcc4 2. \'Testing implementation details\' (JavaScript example)\\n\\n**\u270d\ufe0f Author:** Kent C Dodds\\n\\n**\ud83d\udd16 Abstract:** The author outlines with a code example the unavoidable tragic faith of a tester who assert on implementation details. Put aside the effort in testing so many details, going this route always end with \'false positive\' and \'false negative\' that clouds the tests reliability. The article illustrate this with a frontend code example but the lesson takeaway is ubiquitous to any kind of testing\\n\\n> \\"There are two distinct reasons that it\'s important to avoid testing implementation details. Tests which test implementation details: \\n> 1. Can break when you refactor application code. *False negatives*\\n> 2. May not fail when you break application code. *False positives*\\"\\n\\n\\np.s. This author has another outstanding post about a modern testing strategy, checkout this one as well - [\'Write tests. Not too many. Mostly integration\'](https://kentcdodds.com/blog/write-tests)\\n\\n\\n**\ud83d\udc53 Read time:** 13 min (2600 words)\\n\\n**\ud83d\udd17 Link:** [https://kentcdodds.com/blog/testing-implementation-details](https://kentcdodds.com/blog/testing-implementation-details)\\n\\n  <br/>\\n\\n## \ud83d\udcc4 3. \'Testing Microservices, the sane way\'\\n\\n\ud83c\udfc5 This is a masterpiece\\n\\n**\u270d\ufe0f Author:** Cindy Sridharan\\n\\n**\ud83d\udd16 Abstract:** This one is the entire Microservices and distributed modern testing bible packed in a single long article that is also super engaging. I remember when came across it four years ago, winter time, I spent an hour everyday under my blanket before sleep with a smile is spread over my face. I clicked on every link, pause after every paragraph to think - a whole new world was opening in front of me. In fact, it was so fascinating that it made me want to specialize in this domain. Fast forward, years later, this is a major part of my work and I enjoy every moment\\n\\nThis paper starts by explaining why E2E, unit tests and explanatory QA will fall short in a distributed environment. Not only this, why any kind of coded test won\'t be enough and a rich toolbox of techniques is needed. It goes through a handful of modern testing techniques that are unfamiliar to most developers. One of its key parts deal with what should be the canonical developer\'s testing technique: the author advocates for \\"big unit tests\\" (i.e., component tests) as it strikes a great balance between developers comfort and realism\\n\\n> I coined the term \u201cstep-up testing\u201d, the general idea being to test at one layer above what\u2019s generally advocated for. Under this model, unit tests would look more like integration tests (by treating I/O as a part of the unit under test within a bounded context), integration testing would look more like testing against real production, and testing in production looks more like, well, monitoring and exploration. The restructured test pyramid (test funnel?) for distributed systems would look like the following:\\n\\n![When unit shines](./spectrum-of-testing.png)\\n\\nBeyond its main scope, whatever type of system you are dealing with - this article will broaden yours perspective on testing and expose you to many new ideas that are highly applicable\\n\\n\\n**\ud83d\udc53 Read time:** > 2 hours (10,500 words with many links)\\n\\n**\ud83d\udd17 Link:** [https://copyconstruct.medium.com/testing-microservices-the-sane-way-9bb31d158c16](https://copyconstruct.medium.com/testing-microservices-the-sane-way-9bb31d158c16)\\n\\n<br/>\\n\\n## \ud83d\udcc4 4. \'How to Unit Test with Node.js?\' (JavaScript examples, for beginners)\\n\\n**\u270d\ufe0f Author:** Ryan Jones\\n\\n**\ud83d\udd16 Abstract:** *One single recommendation for beginners:* Any other article on this list covers advanced testing. This article, and only this one, is meant for testing newbies who are looking to take their first practical steps in this world\\n\\nThis tutorial was chosen from a handful of other alternatives because it\'s well-written and also relatively comprehensive. It covers the first steps \'kata\' that a beginner should learn first about: the test anatomy syntax, test runners CLI, assertions and asynchronous tests. Goes without words, this knowledge won\'t be sufficient for covering a real-world app with testing, but it gets you safely to the next phase. My personal advice: after reading this one, your next step is learning about [test doubles (mocking)](https://www.testim.io/blog/sinon-js-tutorial/)\\n\\n**\ud83d\udc53 Read time:** 16 min (3000 words)\\n\\n**\ud83d\udd17 Link:** [https://medium.com/serverlessguru/how-to-unit-test-with-nodejs-76967019ba56](https://medium.com/serverlessguru/how-to-unit-test-with-nodejs-76967019ba56)\\n\\n<br/>\\n\\n## \ud83d\udcc4 5. \'Unit test fetish\'\\n\\n**\u270d\ufe0f Author:** Martin S\xfastrik\\n\\n**\ud83d\udd16 Abstract:** The article opens with \'I hear that prople feel an uncontrollable urge to write unit tests nowaydays. If you are one of those affected, spare few minutes and consider these reasons for NOT writing unit tests\'. Despite these words, the article is not against unit tests as a principle rather highlights when & where unit tests fall short. In these cases, other techniques should be considered. Here is an example: Unit tests inherently have lower return on investment, the author comes with a sounding analogy for this: \'If you are painting a house, you want to start with a biggest brush at hand and spare the tiny brush for the end to deal with fine details. If you begin your QA work with unit tests, you are essentially trying to paint entire house using the finest chinese calligraphy brush...\'\\n\\n**\ud83d\udc53 Read time:** 5 min (1000 words)\\n\\n**\ud83d\udd17 Link:** [https://250bpm.com/blog:40/](https://250bpm.com/blog:40/)\\n\\n<br/>\\n\\n## \ud83d\udcc4 6. \'Mocking is a Code Smell\' (JavaScript examples)\\n\\n**\u270d\ufe0f Author:** Eric Elliott\\n\\n**\ud83d\udd16 Abstract:** Most of the articles here belong more to the \'modern wave of testing\', here is something more \'classic\' and appealing to TDD lovers or just anyone with a need to write unit tests. This article is about HOW to reduce the number of mocking (test doubles) in your tests. Not only because mocking is an overhead in test writing, also because they hint that something might be wrong. In other words, mocking is not definitely wrong and must be fixed right away but *many* mocking are a sign of something not ideal. Consider a module that inherits from many others, or a chatty one that collaborates with a handful of other modules to do its job - testing and changing this structure is a burden:\\n\\n\\n> \\"Mocking is required when our decomposition strategy has failed\\"\\n\\nThe author goes through a various of techniques to design more autonomous units like using pure functions by isolating side-effects from the rest of the program logic, using pub/sub, isolating I/O, composing units with patterns like monadic compositions, and some more\\n\\nThe overall article tone is balanced. In some parts, it encourages functional programming and techniques that are far from the mainstream - consider reading these few parts with a grain of salt\\n\\n**\ud83d\udc53 Read time:** 32 min (6,300 words)\\n\\n**\ud83d\udd17 Link:** [https://medium.com/javascript-scene/mocking-is-a-code-smell-944a70c90a6a](https://medium.com/javascript-scene/mocking-is-a-code-smell-944a70c90a6a)\\n\\n<br/>\\n\\n## \ud83d\udcc4 7. \'Why Good Developers Write Bad Unit Tests\'\\n\\n\ud83c\udfc5 This is a masterpiece\\n\\n**\u270d\ufe0f Author:** Michael Lynch\\n\\n**\ud83d\udd16 Abstract:** I love this one so much. The author exemplifies how *unexpectedly* it is sometimes the good developers with their great intentions who write bad tests:\\n\\n> Too often, software developers approach unit testing with the same flawed thinking... They mechanically apply all the \u201crules\u201d they learned in production code without examining whether they\u2019re appropriate for tests. As a result, they build skyscrapers at the beach\\n\\nConcrete code examples show how the test readability deteriorates once we apply \'sky scrapper\' thinking and how to keep it simple. In one part, he demonstrates how violating the DRY principle thoughtfully allows the reader to stay within the test while still keeping the code maintainable. This article alone, in 11 minutes, can greatly improve the tests of developers who tend to write sophisticated tests. If you have someone like this in your team, you now know what to do\\n\\n**\ud83d\udc53 Read time:** 11 min (2,2000 words)\\n\\n**\ud83d\udd17 Link:** [https://mtlynch.io/good-developers-bad-tests/](https://mtlynch.io/good-developers-bad-tests/)\\n\\n<br/>\\n\\n## \ud83d\udcc4 8. \'An Overview of JavaScript Testing in 2022\' (JavaScript examples)\\n\\n**\u270d\ufe0f Author:** Vitali Zaidman\\n\\n**\ud83d\udd16 Abstract:** This paper is unique here as it doesn\'t cover a single topic rather being a rundown of (almost) all JavaScript testing tools. This allows you to enrich the toolbox in your mind, and have more screwdrivers for more type of screws. For example, knowing that there are IDE extensions that shows coverage information right within the code might help you boost the tests adoption in the team, if needed. Knowing that there are solid, free, and open source visual regression tools might encourage you to dip your toes in this water, to name a few examples.\\n\\n> \\"We reviewed the most trending testing strategies and tools in the web development community and hopefully made it easier for you to test your sites. In the end, the best decisions regarding application architecture today are made by understanding general patterns that are trending in the very active community of developers, and combining them with your own experience and the characteristics of your application.\\"\\n\\n The author was also kind enough to leave pros/cons nearby most tools so the reader can quickly get a sense of how the various options stack with each other. The article covers categories like assertion libraries, test runners, code coverage tools, visual regression tools, E2E suits and more\\n\\n**\ud83d\udc53 Read time:** 37 min (7,400 words)\\n\\n**\ud83d\udd17 Link:** [https://medium.com/welldone-software/an-overview-of-javascript-testing-7ce7298b9870](https://medium.com/welldone-software/an-overview-of-javascript-testing-7ce7298b9870)\\n\\n<br/>\\n\\n## \ud83d\udcc4 9. Testing in Production, the safe way\\n\\n**\u270d\ufe0f Author:** Cindy Sridharan\\n\\n**\ud83d\udd16 Abstract:** \'Testing in production\' is a provocative term that sounds like a risky and careless approach of testing over production instead of verifying the delivery beforehand (yet another case of bad testing terminology). In practice, testing in production doesn\'t replace coding-time testing, it just add _additional_ layer of confidence by _safely_ testing in 3 more phases: deployment, release and post-release. This comprehensive article covers dozens of techniques, some are unusual like traffic shadowing, tap compare and more. More than anything else, it illustrates an holistic testing workflow, build confidence cumulatively from developer machine until the new version is serving users in production\\n\\n> I\u2019m more and more convinced that staging environments are like mocks - at best a pale imitation of the genuine article and the worst form of confirmation bias. \\n\\n> It\u2019s still better than having nothing - but \u201cworks in staging\u201d is only one step better than \u201cworks on my machine\u201d.\\n\\n![Testing in production](./the-3-phases.jpeg)\\n\\n**\ud83d\udc53 Read time:** 54 min (10,725 words)\\n\\n**\ud83d\udd17 Link:** [https://copyconstruct.medium.com/testing-in-production-the-safe-way-18ca102d0ef1](https://copyconstruct.medium.com/testing-in-production-the-safe-way-18ca102d0ef1)\\n\\n<br/>\\n\\n## \ud83d\udcc4 10. \'Please don\'t mock me\' (JavaScript examples, from JSConf)\\n\\n\ud83c\udfc5 This is a masterpiece\\n\\n**\u270d\ufe0f Author:** Justin Searls \\n\\n**\ud83d\udd16 Abstract:** This fantastic YouTube deals with the Achilles heel of testing: where exactly to mock. The dilemma where to end the test scope, what should be mocked and what\'s not - is presumably the most strategic test design decision. Consider for example having module A which interacts with module B. If you isolate A by mocking B, A will always pass, even when B\'s interface has changed and A\'s code didn\'t follow. This makes A\'s tests highly stable but... production will fail in hours. In his talk Justin says:\\n\\n> \\"A test that never fails is a bad test because it doesn\'t tell you anything. Design tests to fail\\"\\n\\nThen he goes and tackle many other interesting mocking crossroads, with beautiful visuals, tons of insights. Please don\'t miss this one\\n\\n**\ud83d\udc53 Read time:** 39 min\\n\\n**\ud83d\udd17 Link:** [https://www.youtube.com/watch?v=x8sKpJwq6lY&list=PL1CRgzydk3vzk5nMZNLTODfMartQQzInE&index=148](https://www.youtube.com/watch?v=x8sKpJwq6lY&list=PL1CRgzydk3vzk5nMZNLTODfMartQQzInE&index=148)\\n\\n<br/>\\n\\n### \ud83d\udcc4 Shameless plug: my articles\\n\\nHere are a few articles that I wrote, obviously I don\'t \'recommend\' my own craft, just checking modestly whether they appeal to you. Together, these articles gained 25,000 GitHub stars, maybe you\'ll find one of them them useful?\\n\\n* [Node.js testing - beyond the basics](https://github.com/testjavascript/nodejs-integration-tests-best-practices)\\n* [50+ JavaScript testing best practices](https://github.com/goldbergyoni/javascript-testing-best-practices)\\n* [Writing clean JavaScript tests](https://yonigoldberg.medium.com/fighting-javascript-tests-complexity-with-the-basic-principles-87b7622eac9a)\\n\\n### \ud83c\udf81 Bonus: Some other great testing content\\n\\nThese articles are also great, some are highly popular:\\n\\n* [Property-Based Testing for everyone](https://www.youtube.com/watch?v=5pwv3cuo3Qk)\\n* [METAMORPHIC TESTING](https://www.hillelwayne.com/post/metamorphic-testing/)\\n* [Lean Testing or Why Unit Tests are Worse than You Think](https://medium.com/@eugenkiss/lean-testing-or-why-unit-tests-are-worse-than-you-think-b6500139a009)\\n* [Testing Strategies in a Microservice Architecture](https://martinfowler.com/articles/microservice-testing/?utm_source=pocket_saves)\\n* [Test Desiderata](https://kentbeck.github.io/TestDesiderata/)\\n* [TDD is dead. Long live testing](https://dhh.dk/2014/tdd-is-dead-long-live-testing.html)\\n* [Test-induced-design-damage](https://dhh.dk/2014/test-induced-design-damage.html)\\n* [testing-without-mocks](https://www.jamesshore.com/v2/projects/nullables/testing-without-mocks)\\n* [Testing Node.js error handling](https://blog.developer.adobe.com/testing-error-handling-in-node-js-567323397114)\\n\\np.s. Last reminder, less than 48 hours left for my [online course \ud83c\udf81 special launch offer](https://courses.testjavascript.com/p/node-js-javascript-testing-from-a-to-z)"},{"id":"testing-the-dark-scenarios-of-your-nodejs-application","metadata":{"permalink":"/blog/testing-the-dark-scenarios-of-your-nodejs-application","editUrl":"https://github.com/practicajs/practica/tree/main/docs/blog/crucial-tests/index.md","source":"@site/blog/crucial-tests/index.md","title":"Testing the dark scenarios of your Node.js application","description":"Where the dead-bodies are covered","date":"2023-07-07T11:00:00.000Z","formattedDate":"July 7, 2023","tags":[{"label":"node.js","permalink":"/blog/tags/node-js"},{"label":"testing","permalink":"/blog/tags/testing"},{"label":"component-test","permalink":"/blog/tags/component-test"},{"label":"fastify","permalink":"/blog/tags/fastify"},{"label":"unit-test","permalink":"/blog/tags/unit-test"},{"label":"integration","permalink":"/blog/tags/integration"},{"label":"nock","permalink":"/blog/tags/nock"}],"readingTime":20.25,"hasTruncateMarker":false,"authors":[{"name":"Yoni Goldberg","title":"Practica.js core maintainer","url":"https://github.com/goldbergyoni","imageURL":"https://github.com/goldbergyoni.png","key":"goldbergyoni"},{"name":"Raz Luvaton","title":"Practica.js core maintainer","url":"https://github.com/rluvaton","imageURL":"https://avatars.githubusercontent.com/u/16746759?v=4","key":"razluvaton"}],"frontMatter":{"slug":"testing-the-dark-scenarios-of-your-nodejs-application","date":"2023-07-07T11:00","hide_table_of_contents":true,"title":"Testing the dark scenarios of your Node.js application","authors":["goldbergyoni","razluvaton"],"tags":["node.js","testing","component-test","fastify","unit-test","integration","nock"]},"prevItem":{"title":"A compilation of outstanding testing articles (with JavaScript)","permalink":"/blog/a-compilation-of-outstanding-testing-articles-with-javaScript"},"nextItem":{"title":"Practica v0.0.6 is alive","permalink":"/blog/practica-v0.0.6-is-alive"}},"content":"## Where the dead-bodies are covered\\n\\nThis post is about tests that are easy to write, 5-8 lines typically, they cover dark and dangerous corners of our applications, but are often overlooked\\n\\nSome context first: How do we test a modern backend? With [the testing diamond](https://ritesh-kapoor.medium.com/testing-automation-what-are-pyramids-and-diamonds-67494fec7c55), of course, by putting the focus on component/integration tests that cover all the layers, including a real DB. With this approach, our tests 99% resemble the production and the user flows, while the development experience is almost as good as with unit tests. Sweet. If this topic is of interest, we\'ve also written [a guide with 50 best practices for integration tests in Node.js](https://github.com/testjavascript/nodejs-integration-tests-best-practices)\\n\\nBut there is a pitfall: most developers write _only_ semi-happy test cases that are focused on the core user flows. Like invalid inputs, CRUD operations, various application states, etc. This is indeed the bread and butter, a great start, but a whole area is left uncovered. For example, typical tests don\'t simulate an unhandled promise rejection that leads to process crash, nor do they simulate the webserver bootstrap phase that might fail and leave the process idle, or HTTP calls to external services that often end with timeouts and retries. They typically not covering the health and readiness route, nor the integrity of the OpenAPI to the actual routes schema, to name just a few examples. There are many dead bodies covered beyond business logic, things that sometimes are even beyond bugs but rather are concerned with application downtime\\n\\n![The hidden corners](./the-hidden-corners.png)\\n\\nHere are a handful of examples that might open your mind to a whole new class of risks and tests\\n\\n**July 2023: My testing course was launched: I\'ve just released a comprehensive testing course that I\'ve been working on for two years. \ud83c\udf81 It\'s now on sale, but only for the month of July. Check it out at [testjavascript.com](https://testjavascript.com/)**\\n\\n## **Test Examples**\\n\\n## \ud83e\udddf\u200d\u2640\ufe0f The zombie process test\\n\\n**\ud83d\udc49What & so what? -** In all of your tests, you assume that the app has already started successfully, lacking a test against the initialization flow. This is a pity because this phase hides some potential catastrophic failures: First, initialization failures are frequent - many bad things can happen here, like a DB connection failure or a new version that crashes during deployment. For this reason, runtime platforms (like Kubernetes and others) encourage components to signal when they are ready (see [readiness probe](https://komodor.com/learn/kubernetes-readiness-probes-a-practical-guide/#:~:text=A%20readiness%20probe%20allows%20Kubernetes,on%20deletion%20of%20a%20pod.)). Errors at this stage also have a dramatic effect over the app health - if the initialization fails and the process stays alive, it becomes a \'zombie process\'. In this scenario, the runtime platform won\'t realize that something went bad, forward traffic to it and avoid creating alternative instances. Besides exiting gracefully, you may want to consider logging, firing a metric, and adjusting your /readiness route. Does it work? only test can tell!\\n\\n**\ud83d\udcdd Code**\\n\\n**Code under test, api.js:**\\n\\n```javascript\\n// A common express server initialization\\nconst startWebServer = () => {\\n  return new Promise((resolve, reject) => {\\n    try {\\n      // A typical Express setup\\n      expressApp = express();\\n      defineRoutes(expressApp); // a function that defines all routes\\n      expressApp.listen(process.env.WEB_SERVER_PORT);\\n    } catch (error) {\\n      //log here, fire a metric, maybe even retry and finally:\\n      process.exit();\\n    }\\n  });\\n};\\n```\\n\\n**The test:**\\n\\n```javascript\\nconst api = require(\'./entry-points/api\'); // our api starter that exposes \'startWebServer\' function\\nconst sinon = require(\'sinon\'); // a mocking library\\n\\ntest(\'When an error happens during the startup phase, then the process exits\', async () => {\\n  // Arrange\\n  const processExitListener = sinon.stub(process, \'exit\');\\n  // \ud83d\udc47 Choose a function that is part of the initialization phase and make it fail\\n  sinon\\n    .stub(routes, \'defineRoutes\')\\n    .throws(new Error(\'Cant initialize connection\'));\\n\\n  // Act\\n  await api.startWebServer();\\n\\n  // Assert\\n  expect(processExitListener.called).toBe(true);\\n});\\n```\\n  \\n## \ud83d\udc40 The observability test\\n\\n**\ud83d\udc49What & why -** For many, testing error means checking the exception type or the API response. This leaves one of the most essential parts uncovered - making the error **correctly observable**. In plain words, ensuring that it\'s being logged correctly and exposed to the monitoring system. It might sound like an internal thing, implementation testing, but actually, it goes directly to a user. Yes, not the end-user, but rather another important one - the ops user who is on-call. What are the expectations of this user? At the very basic level, when a production issue arises, she must see detailed log entries, _including stack trace_, cause and other properties. This info can save the day when dealing with production incidents. On to of this, in many systems, monitoring is managed separately to conclude about the overall system state using cumulative heuristics (e.g., an increase in the number of errors over the last 3 hours). To support this monitoring needs, the code also must fire error metrics. Even tests that do try to cover these needs take a naive approach by checking that the logger function was called - but hey, does it include the right data? Some write better tests that check the error type that was passed to the logger, good enough? No! The ops user doesn\'t care about the JavaScript class names but the JSON data that is sent out. The following test focuses on the specific properties that are being made observable:\\n\\n**\ud83d\udcdd Code**\\n\\n```javascript\\ntest(\'When exception is throw during request, Then logger reports the mandatory fields\', async () => {\\n  //Arrange\\n  const orderToAdd = {\\n    userId: 1,\\n    productId: 2,\\n    status: \'approved\',\\n  };\\n  const metricsExporterDouble = sinon.stub(metricsExporter, \'fireMetric\');\\n  sinon\\n    .stub(OrderRepository.prototype, \'addOrder\')\\n    .rejects(new AppError(\'saving-failed\', \'Order could not be saved\', 500));\\n  const loggerDouble = sinon.stub(logger, \'error\');\\n\\n  //Act\\n  await axiosAPIClient.post(\'/order\', orderToAdd);\\n\\n  //Assert\\n  expect(loggerDouble).toHaveBeenCalledWith({\\n    name: \'saving-failed\',\\n    status: 500,\\n    stack: expect.any(String),\\n    message: expect.any(String),\\n  });\\n  expect(\\n    metricsExporterDouble).toHaveBeenCalledWith(\'error\', {\\n      errorName: \'example-error\',\\n    })\\n});\\n```\\n\\n## \ud83d\udc7d The \'unexpected visitor\' test - when an uncaught exception meets our code\\n\\n**\ud83d\udc49What & why -** A typical error flow test falsely assumes two conditions: A valid error object was thrown, and it was caught. Neither is guaranteed, let\'s focus on the 2nd assumption: it\'s common for certain errors to left uncaught. The error might get thrown before your framework error handler is ready, some npm libraries can throw surprisingly from different stacks using timer functions, or you just forget to set someEventEmitter.on(\'error\', ...). To name a few examples. These errors will find their way to the global process.on(\'uncaughtException\') handler, **hopefully if your code subscribed**. How do you simulate this scenario in a test? naively you may locate a code area that is not wrapped with try-catch and stub it to throw during the test. But here\'s a catch22: if you are familiar with such area - you are likely to fix it and ensure its errors are caught. What do we do then? we can bring to our benefit the fact the JavaScript is \'borderless\', if some object can emit an event, we as its subscribers can make it emit this event ourselves, here\'s an example:\\n\\nresearches says that, rejection\\n\\n**\ud83d\udcdd Code**\\n\\n```javascript\\ntest(\'When an unhandled exception is thrown, then process stays alive and the error is logged\', async () => {\\n  //Arrange\\n  const loggerDouble = sinon.stub(logger, \'error\');\\n  const processExitListener = sinon.stub(process, \'exit\');\\n  const errorToThrow = new Error(\'An error that wont be caught \ud83d\ude33\');\\n\\n  //Act\\n  process.emit(\'uncaughtException\', errorToThrow); //\ud83d\udc48 Where the magic is\\n\\n  // Assert\\n  expect(processExitListener.called).toBe(false);\\n  expect(loggerDouble).toHaveBeenCalledWith(errorToThrow);\\n});\\n```\\n\\n## \ud83d\udd75\ud83c\udffc The \'hidden effect\' test - when the code should not mutate at all\\n\\n**\ud83d\udc49What & so what -** In common scenarios, the code under test should stop early like when the incoming payload is invalid or a user doesn\'t have sufficient credits to perform an operation. In these cases, no DB records should be mutated. Most tests out there in the wild settle with testing the HTTP response only - got back HTTP 400? great, the validation/authorization probably work. Or does it? The test trusts the code too much, a valid response doesn\'t guarantee that the code behind behaved as design. Maybe a new record was added although the user has no permissions? Clearly you need to test this, but how would you test that a record was NOT added? There are two options here: If the DB is purged before/after every test, than just try to perform an invalid operation and check that the DB is empty afterward. If you\'re not cleaning the DB often (like me, but that\'s another discussion), the payload must contain some unique and queryable value that you can query later and hope to get no records. This is how it looks like:\\n\\n**\ud83d\udcdd Code**\\n\\n```javascript\\nit(\'When adding an invalid order, then it returns 400 and NOT retrievable\', async () => {\\n  //Arrange\\n  const orderToAdd = {\\n    userId: 1,\\n    mode: \'draft\',\\n    externalIdentifier: uuid(), //no existing record has this value\\n  };\\n\\n  //Act\\n  const { status: addingHTTPStatus } = await axiosAPIClient.post(\\n    \'/order\',\\n    orderToAdd\\n  );\\n\\n  //Assert\\n  const { status: fetchingHTTPStatus } = await axiosAPIClient.get(\\n    `/order/externalIdentifier/${orderToAdd.externalIdentifier}`\\n  ); // Trying to get the order that should have failed\\n  expect({ addingHTTPStatus, fetchingHTTPStatus }).toMatchObject({\\n    addingHTTPStatus: 400,\\n    fetchingHTTPStatus: 404,\\n  });\\n  // \ud83d\udc46 Check that no such record exists\\n});\\n```\\n\\n## \ud83e\udde8 The \'overdoing\' test - when the code should mutate but it\'s doing too much\\n\\n**\ud83d\udc49What & why -** This is how a typical data-oriented test looks like: first you add some records, then approach the code under test, and finally assert what happens to these specific records. So far, so good. There is one caveat here though: since the test narrows it focus to specific records, it ignores whether other record were unnecessarily affected. This can be really bad, here\'s a short real-life story that happened to my customer: Some data access code changed and incorporated a bug that updates ALL the system users instead of just one. All test pass since they focused on a specific record which positively updated, they just ignored the others. How would you test and prevent? here is a nice trick that I was taught by my friend Gil Tayar: in the first phase of the test, besides the main records, add one or more \'control\' records that should not get mutated during the test. Then, run the code under test, and besides the main assertion, check also that the control records were not affected:\\n\\n**\ud83d\udcdd Code**\\n\\n```javascript\\ntest(\'When deleting an existing order, Then it should NOT be retrievable\', async () => {\\n  // Arrange\\n  const orderToDelete = {\\n    userId: 1,\\n    productId: 2,\\n  };\\n  const deletedOrder = (await axiosAPIClient.post(\'/order\', orderToDelete)).data\\n    .id; // We will delete this soon\\n  const orderNotToBeDeleted = orderToDelete;\\n  const notDeletedOrder = (\\n    await axiosAPIClient.post(\'/order\', orderNotToBeDeleted)\\n  ).data.id; // We will not delete this\\n\\n  // Act\\n  await axiosAPIClient.delete(`/order/${deletedOrder}`);\\n\\n  // Assert\\n  const { status: getDeletedOrderStatus } = await axiosAPIClient.get(\\n    `/order/${deletedOrder}`\\n  );\\n  const { status: getNotDeletedOrderStatus } = await axiosAPIClient.get(\\n    `/order/${notDeletedOrder}`\\n  );\\n  expect(getNotDeletedOrderStatus).toBe(200);\\n  expect(getDeletedOrderStatus).toBe(404);\\n});\\n```\\n\\n## \ud83d\udd70 The \'slow collaborator\' test - when the other HTTP service times out\\n\\n**\ud83d\udc49What & why -** When your code approaches other services/microservices via HTTP, savvy testers minimize end-to-end tests because these tests lean toward happy paths (it\'s harder to simulate scenarios). This mandates using some mocking tool to act like the remote service, for example, using tools like [nock](https://github.com/nock/nock) or [wiremock](https://wiremock.org/). These tools are great, only some are using them naively and check mainly that calls outside were indeed made. What if the other service is not available **in production**, what if it is slower and times out occasionally (one of the biggest risks of Microservices)? While you can\'t wholly save this transaction, your code should do the best given the situation and retry, or at least log and return the right status to the caller. All the network mocking tools allow simulating delays, timeouts and other \'chaotic\' scenarios. Question left is how to simulate slow response without having slow tests? You may use [fake timers](https://sinonjs.org/releases/latest/fake-timers/) and trick the system into believing as few seconds passed in a single tick. If you\'re using [nock](https://github.com/nock/nock), it offers an interesting feature to simulate timeouts **quickly**: the .delay function simulates slow responses, then nock will realize immediately if the delay is higher than the HTTP client timeout and throw a timeout event immediately without waiting\\n\\n**\ud83d\udcdd Code**\\n\\n```javascript\\n// In this example, our code accepts new Orders and while processing them approaches the Users Microservice\\ntest(\'When users service times out, then return 503 (option 1 with fake timers)\', async () => {\\n  //Arrange\\n  const clock = sinon.useFakeTimers();\\n  config.HTTPCallTimeout = 1000; // Set a timeout for outgoing HTTP calls\\n  nock(`${config.userServiceURL}/user/`)\\n    .get(\'/1\', () => clock.tick(2000)) // Reply delay is bigger than configured timeout \ud83d\udc46\\n    .reply(200);\\n  const loggerDouble = sinon.stub(logger, \'error\');\\n  const orderToAdd = {\\n    userId: 1,\\n    productId: 2,\\n    mode: \'approved\',\\n  };\\n\\n  //Act\\n  // \ud83d\udc47try to add new order which should fail due to User service not available\\n  const response = await axiosAPIClient.post(\'/order\', orderToAdd);\\n\\n  //Assert\\n  // \ud83d\udc47At least our code does its best given this situation\\n  expect(response.status).toBe(503);\\n  expect(loggerDouble.lastCall.firstArg).toMatchObject({\\n    name: \'user-service-not-available\',\\n    stack: expect.any(String),\\n    message: expect.any(String),\\n  });\\n});\\n```\\n\\n## \ud83d\udc8a The \'poisoned message\' test - when the message consumer gets an invalid payload that might put it in stagnation\\n\\n**\ud83d\udc49What & so what -** When testing flows that start or end in a queue, I bet you\'re going to bypass the message queue layer, where the code and libraries consume a queue, and you approach the logic layer directly. Yes, it makes things easier but leaves a class of uncovered risks. For example, what if the logic part throws an error or the message schema is invalid but the message queue consumer fails to translate this exception into a proper message queue action? For example, the consumer code might fail to reject the message or increment the number of attempts (depends on the type of queue that you\'re using). When this happens, the message will enter a loop where it always served again and again. Since this will apply to many messages, things can get really bad as the queue gets highly saturated. For this reason this syndrome was called the \'poisoned message\'. To mitigate this risk, the tests\' scope must include all the layers like how you probably do when testing against APIs. Unfortunately, this is not as easy as testing with DB because message queues are flaky, here is why\\n\\nWhen testing with real queues things get curios and curiouser: tests from different process will steal messages from each other, purging queues is harder that you might think (e.g. [SQS demand 60 seconds](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-purge-queue.html) to purge queues), to name a few challenges that you won\'t find when dealing with real DB\\n\\nHere is a strategy that works for many teams and holds a small compromise - use a fake in-memory message queue. By \'fake\' I mean something simplistic that acts like a stub/spy and do nothing but telling when certain calls are made (e.g., consume, delete, publish). You might find reputable fakes/stubs for your own message queue like [this one for SQS](https://github.com/m-radzikowski/aws-sdk-client-mock) and you can code one **easily** yourself. No worries, I\'m not a favour of maintaining myself testing infrastructure, this proposed component is extremely simply and unlikely to surpass 50 lines of code (see example below). On top of this, whether using a real or fake queue, one more thing is needed: create a convenient interface that tells to the test when certain things happened like when a message was acknowledged/deleted or a new message was published. Without this, the test never knows when certain events happened and lean toward quirky techniques like polling. Having this setup, the test will be short, flat and you can easily simulate common message queue scenarios like out of order messages, batch reject, duplicated messages and in our example - the poisoned message scenario (using RabbitMQ):\\n\\n**\ud83d\udcdd Code**\\n\\n1. Create a fake message queue that does almost nothing but record calls, see full example here\\n\\n```javascript\\nclass FakeMessageQueueProvider extends EventEmitter {\\n  // Implement here\\n\\n  publish(message) {}\\n\\n  consume(queueName, callback) {}\\n}\\n```\\n\\n2. Make your message queue client accept real or fake provider\\n\\n```javascript\\nclass MessageQueueClient extends EventEmitter {\\n  // Pass to it a fake or real message queue\\n  constructor(customMessageQueueProvider) {}\\n\\n  publish(message) {}\\n\\n  consume(queueName, callback) {}\\n\\n  // Simple implementation can be found here:\\n  // https://github.com/testjavascript/nodejs-integration-tests-best-practices/blob/master/example-application/libraries/fake-message-queue-provider.js\\n}\\n```\\n\\n3. Expose a convenient function that tells when certain calls where made\\n\\n```javascript\\nclass MessageQueueClient extends EventEmitter {\\n  publish(message) {}\\n\\n  consume(queueName, callback) {}\\n\\n  // \ud83d\udc47\\n  waitForEvent(eventName: \'publish\' | \'consume\' | \'acknowledge\' | \'reject\', howManyTimes: number) : Promise\\n}\\n```\\n\\n4. The test is now short, flat and expressive \ud83d\udc47\\n\\n```javascript\\nconst FakeMessageQueueProvider = require(\'./libs/fake-message-queue-provider\');\\nconst MessageQueueClient = require(\'./libs/message-queue-client\');\\nconst newOrderService = require(\'./domain/newOrderService\');\\n\\ntest(\'When a poisoned message arrives, then it is being rejected back\', async () => {\\n  // Arrange\\n  const messageWithInvalidSchema = { nonExistingProperty: \'invalid\u274c\' };\\n  const messageQueueClient = new MessageQueueClient(\\n    new FakeMessageQueueProvider()\\n  );\\n  // Subscribe to new messages and passing the handler function\\n  messageQueueClient.consume(\'orders.new\', newOrderService.addOrder);\\n\\n  // Act\\n  await messageQueueClient.publish(\'orders.new\', messageWithInvalidSchema);\\n  // Now all the layers of the app will get stretched \ud83d\udc46, including logic and message queue libraries\\n\\n  // Assert\\n  await messageQueueClient.waitFor(\'reject\', { howManyTimes: 1 });\\n  // \ud83d\udc46 This tells us that eventually our code asked the message queue client to reject this poisoned message\\n});\\n```\\n\\n**\ud83d\udcddFull code example -** [is here](https://github.com/testjavascript/nodejs-integration-tests-best-practices/blob/master/recipes/message-queue/fake-message-queue.test.js)\\n\\n## \ud83d\udce6 Test the package as a consumer\\n\\n**\ud83d\udc49What & why -** When publishing a library to npm, easily all your tests might pass BUT... the same functionality will fail over the end-user\'s computer. How come? tests are executed against the local developer files, but the end-user is only exposed to artifacts _that were built_. See the mismatch here? _after_ running the tests, the package files are transpiled (I\'m looking at you babel users), zipped and packed. If a single file is excluded due to .npmignore or a polyfill is not added correctly, the published code will lack mandatory files\\n\\n**\ud83d\udcdd Code**\\n\\nConsider the following scenario, you\'re developing a library, and you wrote this code:\\n```js\\n// index.js\\nexport * from \'./calculate.js\';\\n\\n// calculate.js \ud83d\udc48\\nexport function calculate() {\\n  return 1;\\n}\\n```\\n\\nThen some tests:\\n```js\\nimport { calculate } from \'./index.js\';\\n\\ntest(\'should return 1\', () => {\\n  expect(calculate()).toBe(1);\\n})\\n\\n\u2705 All tests pass \ud83c\udf8a\\n```\\n\\nFinally configure the package.json:\\n```json5\\n{\\n  // ....\\n  \\"files\\": [\\n    \\"index.js\\"\\n  ]\\n}\\n```\\n\\nSee, 100% coverage, all tests pass locally and in the CI \u2705, it just won\'t work in production \ud83d\udc79. Why? because you forgot to include the `calculate.js` in the package.json `files` array \ud83d\udc46\\n\\n\\nWhat can we do instead? we can test the library as _its end-users_. How? publish the package to a local registry like [verdaccio](https://verdaccio.org/), let the tests install and approach the *published* code. Sounds troublesome? judge yourself \ud83d\udc47\\n\\n**\ud83d\udcdd Code**\\n\\n```js\\n// global-setup.js\\n\\n// 1. Setup the in-memory NPM registry, one function that\'s it! \ud83d\udd25\\nawait setupVerdaccio();\\n\\n// 2. Building our package \\nawait exec(\'npm\', [\'run\', \'build\'], {\\n    cwd: packagePath,\\n});\\n\\n// 3. Publish it to the in-memory registry\\nawait exec(\'npm\', [\'publish\', \'--registry=http://localhost:4873\'], {\\n    cwd: packagePath,\\n});\\n\\n// 4. Installing it in the consumer directory\\nawait exec(\'npm\', [\'install\', \'my-package\', \'--registry=http://localhost:4873\'], {\\n    cwd: consumerPath,\\n});\\n\\n// Test file in the consumerPath\\n\\n// 5. Test the package \ud83d\ude80\\ntest(\\"should succeed\\", async () => {\\n    const { fn1 } = await import(\'my-package\');\\n\\n    expect(fn1()).toEqual(1);\\n});\\n```\\n\\n**\ud83d\udcddFull code example -** [is here](https://github.com/rluvaton/e2e-verdaccio-example)\\n\\nWhat else this technique can be useful for?\\n\\n- Testing different version of peer dependency you support - let\'s say your package support react 16 to 18, you can now test that\\n- You want to test ESM and CJS consumers\\n- If you have CLI application you can test it like your users\\n- Making sure all the voodoo magic in that babel file is working as expected\\n\\n## \ud83d\uddde The \'broken contract\' test - when the code is great but its corresponding OpenAPI docs leads to a production bug\\n\\n**\ud83d\udc49What & so what -** Quite confidently I\'m sure that almost no team test their OpenAPI correctness. \\"It\'s just documentation\\", \\"we generate it automatically based on code\\" are typical belief found for this reason. Let me show you how this auto generated documentation can be wrong and lead not only to frustration but also to a bug. In production.\\n\\nConsider the following scenario, you\'re requested to return HTTP error status code if an order is duplicated but forget to update the OpenAPI specification with this new HTTP status response. While some framework can update the docs with new fields, none can realize which errors your code throws, this labour is always manual. On the other side of the line, the API client is doing everything just right, going by the spec that you published, adding orders with some duplication because the docs don\'t forbid doing so. Then, BOOM, production bug -> the client crashes and shows an ugly unknown error message to the user. This type of failure is called the \'contract\' problem when two parties interact, each has a code that works perfect, they just operate under different spec and assumptions. While there are fancy sophisticated and exhaustive solution to this challenge (e.g., [PACT](https://pact.io)), there are also leaner approaches that gets you covered _easily and quickly_ (at the price of covering less risks).\\n\\nThe following sweet technique is based on libraries (jest, mocha) that listen to all network responses, compare the payload against the OpenAPI document, and if any deviation is found - make the test fail with a descriptive error. With this new weapon in your toolbox and almost zero effort, another risk is ticked. It\'s a pity that these libs can\'t assert also against the incoming requests to tell you that your tests use the API wrong. One small caveat and an elegant solution: These libraries dictate putting an assertion statement in every test - expect(response).toSatisfyApiSpec(), a bit tedious and relies on human discipline. You can do better if your HTTP client supports plugin/hook/interceptor by putting this assertion in a single place that will apply in all the tests:\\n\\n**\ud83d\udcdd Code**\\n\\n**Code under test, an API throw a new error status**\\n\\n```javascript\\nif (doesOrderCouponAlreadyExist) {\\n  throw new AppError(\'duplicated-coupon\', { httpStatus: 409 });\\n}\\n```\\n\\nThe OpenAPI doesn\'t document HTTP status \'409\', no framework knows to update the OpenAPI doc based on thrown exceptions\\n\\n```json\\n\\"responses\\": {\\n    \\"200\\": {\\n      \\"description\\": \\"successful\\",\\n      }\\n    ,\\n    \\"400\\": {\\n      \\"description\\": \\"Invalid ID\\",\\n      \\"content\\": {}\\n    },// No 409 in this list\ud83d\ude32\ud83d\udc48\\n}\\n\\n```\\n\\n**The test code**\\n\\n```javascript\\nconst jestOpenAPI = require(\'jest-openapi\');\\njestOpenAPI(\'../openapi.json\');\\n\\ntest(\'When an order with duplicated coupon is added , then 409 error should get returned\', async () => {\\n  // Arrange\\n  const orderToAdd = {\\n    userId: 1,\\n    productId: 2,\\n    couponId: uuid(),\\n  };\\n  await axiosAPIClient.post(\'/order\', orderToAdd);\\n\\n  // Act\\n  // We\'re adding the same coupon twice \ud83d\udc47\\n  const receivedResponse = await axios.post(\'/order\', orderToAdd);\\n\\n  // Assert;\\n  expect(receivedResponse.status).toBe(409);\\n  expect(res).toSatisfyApiSpec();\\n  // This \ud83d\udc46 will throw if the API response, body or status, is different that was it stated in the OpenAPI\\n});\\n```\\n\\nTrick: If your HTTP client supports any kind of plugin/hook/interceptor, put the following code in \'beforeAll\'. This covers all the tests against OpenAPI mismatches\\n\\n```javascript\\nbeforeAll(() => {\\n  axios.interceptors.response.use((response) => {\\n    expect(response.toSatisfyApiSpec());\\n    // With this \ud83d\udc46, add nothing to the tests - each will fail if the response deviates from the docs\\n  });\\n});\\n```\\n\\n## Even more ideas\\n\\n- Test readiness and health routes\\n- Test message queue connection failures\\n- Test JWT and JWKS failures\\n- Test security-related things like CSRF tokens\\n- Test your HTTP client retry mechanism (very easy with nock)\\n- Test that the DB migration succeed and the new code can work with old records format\\n- Test DB connection disconnects\\n  \\n## It\'s not just ideas, it a whole new mindset\\n\\nThe examples above were not meant only to be a checklist of \'don\'t forget\' test cases, but rather a fresh mindset on what tests could cover for you. Modern tests are not just about functions, or user flows, but any risk that might visit your production. This is doable only with component/integration tests but never with unit or end-to-end tests. Why? Because unlike unit you need all the parts to play together (e.g., the DB migration file, with the DAL layer and the error handler all together). Unlike E2E, you have the power to simulate in-process scenarios that demand some tweaking and mocking. Component tests allow you to include many production moving parts early on your machine. I like calling this \'production-oriented development\'\\n\\n**My new online testing course -** If you\'re intrigued with beyond the basics testing patterns, [consider my online course which was just launched and is \ud83c\udf81 on sale for 30 days (July 2023)](https://testjavascript.com)"},{"id":"practica-v0.0.6-is-alive","metadata":{"permalink":"/blog/practica-v0.0.6-is-alive","editUrl":"https://github.com/practicajs/practica/tree/main/docs/blog/v0.6-is-alive/index.md","source":"@site/blog/v0.6-is-alive/index.md","title":"Practica v0.0.6 is alive","description":"Where is our focus now?","date":"2022-12-10T10:00:00.000Z","formattedDate":"December 10, 2022","tags":[{"label":"node.js","permalink":"/blog/tags/node-js"},{"label":"express","permalink":"/blog/tags/express"},{"label":"practica","permalink":"/blog/tags/practica"},{"label":"prisma","permalink":"/blog/tags/prisma"}],"readingTime":1.47,"hasTruncateMarker":false,"authors":[{"name":"Yoni Goldberg","title":"Practica.js core maintainer","url":"https://github.com/goldbergyoni","imageURL":"https://github.com/goldbergyoni.png","key":"goldbergyoni"},{"name":"Raz Luvaton","title":"Practica.js core maintainer","url":"https://github.com/rluvaton","imageURL":"https://avatars.githubusercontent.com/u/16746759?v=4","key":"razluvaton"},{"name":"Daniel Gluskin","title":"Practica.js core maintainer","url":"https://github.com/DanielGluskin","imageURL":"https://avatars.githubusercontent.com/u/17989958?v=4","key":"danielgluskin"},{"name":"Michael Salomon","title":"Practica.js core maintainer","url":"https://github.com/mikicho","imageURL":"https://avatars.githubusercontent.com/u/11459632?v=4","key":"michaelsalomon"}],"frontMatter":{"slug":"practica-v0.0.6-is-alive","date":"2022-12-10T10:00","hide_table_of_contents":true,"title":"Practica v0.0.6 is alive","authors":["goldbergyoni","razluvaton","danielgluskin","michaelsalomon"],"tags":["node.js","express","practica","prisma"]},"prevItem":{"title":"Testing the dark scenarios of your Node.js application","permalink":"/blog/testing-the-dark-scenarios-of-your-nodejs-application"},"nextItem":{"title":"Is Prisma better than your \'traditional\' ORM?","permalink":"/blog/is-prisma-better-than-your-traditional-orm"}},"content":"## Where is our focus now?\\n\\nWe work in two parallel paths: enriching the supported best practices to make the code more production ready and at the same time enhance the existing code based off the community feedback\\n\\n## What\'s new?\\n\\n### Request-level store\\n\\nEvery request now has its own store of variables, you may assign information on the request-level so every code which was called from this specific request has access to these variables. For example, for storing the user permissions. One special variable that is stored is \'request-id\' which is a unique UUID per request (also called correlation-id). The logger automatically will emit this to every log entry. We use the built-in [AsyncLocal](https://nodejs.org/api/async_context.html) for this task\\n\\n### Hardened .dockerfile\\n\\nAlthough a Dockerfile may contain 10 lines, it easy and common to include 20 mistakes in these short artifact. For example, commonly npmrc secrets are leaked, usage of vulnerable base image and other typical mistakes. Our .Dockerfile follows the best practices from [this article](https://snyk.io/blog/10-best-practices-to-containerize-nodejs-web-applications-with-docker/) and already apply 90% of the guidelines\\n\\n### Additional ORM option: Prisma\\n\\nPrisma is an emerging ORM with great type safe support and awesome DX. We will keep Sequelize as our default ORM while Prisma will be an optional choice using the flag: --orm=prisma\\n\\nWhy did we add it to our tools basket and why Sequelize is still the default? We summarized all of our thoughts and data in this [blog post](https://practica.dev/blog/is-prisma-better-than-your-traditional-orm/)\\n\\n### Many small enhancements\\n\\nMore than 10 PR were merged with CLI experience improvements, bug fixes, code patterns enhancements and more\\n\\n## Where do I start?\\n\\nDefinitely follow the [getting started guide first](https://practica.dev/the-basics/getting-started-quickly) and then read the guide [coding with practica](https://practica.dev/the-basics/coding-with-practica) to realize its full power and genuine value. We will be thankful to receive your feedback"},{"id":"is-prisma-better-than-your-traditional-orm","metadata":{"permalink":"/blog/is-prisma-better-than-your-traditional-orm","editUrl":"https://github.com/practicajs/practica/tree/main/docs/blog/is-prisma-better/index.md","source":"@site/blog/is-prisma-better/index.md","title":"Is Prisma better than your \'traditional\' ORM?","description":"Intro - Why discuss yet another ORM (or the man who had a stain on his fancy suite)?","date":"2022-12-07T11:00:00.000Z","formattedDate":"December 7, 2022","tags":[{"label":"node.js","permalink":"/blog/tags/node-js"},{"label":"express","permalink":"/blog/tags/express"},{"label":"nestjs","permalink":"/blog/tags/nestjs"},{"label":"fastify","permalink":"/blog/tags/fastify"},{"label":"passport","permalink":"/blog/tags/passport"},{"label":"dotenv","permalink":"/blog/tags/dotenv"},{"label":"supertest","permalink":"/blog/tags/supertest"},{"label":"practica","permalink":"/blog/tags/practica"},{"label":"testing","permalink":"/blog/tags/testing"}],"readingTime":23.875,"hasTruncateMarker":true,"authors":[{"name":"Yoni Goldberg","title":"Practica.js core maintainer","url":"https://github.com/goldbergyoni","imageURL":"https://github.com/goldbergyoni.png","key":"goldbergyoni"}],"frontMatter":{"slug":"is-prisma-better-than-your-traditional-orm","date":"2022-12-07T11:00","hide_table_of_contents":true,"title":"Is Prisma better than your \'traditional\' ORM?","authors":["goldbergyoni"],"tags":["node.js","express","nestjs","fastify","passport","dotenv","supertest","practica","testing"]},"prevItem":{"title":"Practica v0.0.6 is alive","permalink":"/blog/practica-v0.0.6-is-alive"},"nextItem":{"title":"Which Monorepo is right for a Node.js BACKEND\xa0now?","permalink":"/blog/monorepo-backend"}},"content":"## Intro - Why discuss yet another ORM (or the man who had a stain on his fancy suite)?\\n\\n*Betteridge\'s law of headlines suggests that a \'headline that ends in a question mark can be answered by the word NO\'. Will this article follow this rule?*\\n\\nImagine an elegant businessman (or woman) walking into a building, wearing a fancy tuxedo and a luxury watch wrapped around his palm. He smiles and waves all over to say hello while people around are starring admirably. You get a little closer, then shockingly, while standing nearby it\'s hard ignore a bold a dark stain over his white shirt. What a dissonance, suddenly all of that glamour is stained\\n\\n![Suite with stain](./suite.png)\\n\\n Like this businessman, Node is highly capable and popular, and yet, in certain areas, its offering basket is stained with inferior offerings. One of these areas is the ORM space, \\"I wish we had something like (Java) hibernate or (.NET) Entity Framework\\" are common words being heard by Node developers. What about existing mature ORMs like TypeORM and Sequelize? We owe so much to these maintainers, and yet, the produced developer experience, the level of maintenance - just don\'t feel delightful, some may say even mediocre. At least so I believed *before* writing this article...\\n\\nFrom time to time, a shiny new ORM is launched, and there is hope. Then soon it\'s realized that these new emerging projects are more of the same, if they survive. Until one day, Prisma ORM arrived surrounded with glamour: It\'s gaining tons of attention all over, producing fantastic content, being used by respectful frameworks and... raised 40,000,000$ (40 million) to build the next generation ORM - Is it the \'Ferrari\' ORM we\'ve been waiting for? Is it a game changer? If you\'re are the \'no ORM for me\' type, will this one make you convert your religion?\\n\\nIn [Practica.js](https://github.com/practicajs/practica) (the Node.js starter based off [Node.js best practices with 83,000 stars](https://github.com/goldbergyoni/nodebestpractices)) we aim to make the best decisions for our users, the Prisma hype made us stop by for a second, evaluate its unique offering and conclude whether we should upgrade our toolbox?\\n\\nThis article is certainly not an \'ORM 101\' but rather a spotlight on specific dimensions in which Prisma aims to shine or struggle. It\'s compared against the two most popular Node.js ORM - TypeORM and Sequelize. Why not others? Why other promising contenders like MikroORM weren\'t covered? Just because they are not as popular yet ana maturity is a critical trait of ORMs\\n\\nReady to explore how good Prisma is and whether you should throw away your current tools?\\n\\n\x3c!--truncate--\x3e\\n\\n## TOC\\n\\n1. Prisma basics in 3 minutes\\n2. Things that are mostly the same\\n3. Differentiation\\n4. Closing\\n\\n## Prisma basics in 3 minutes\\n\\nJust before delving into the strategic differences, for the benefit of those unfamiliar with Prisma - here is a quick \'hello-world\' workflow with Prisma ORM. If you\'re already familiar with it - skipping to the next section sounds sensible. Simply put, Prisma dictates 3 key steps to get our ORM code working:\\n\\n**A. Define a model -** Unlike almost any other ORM, Prisma brings a unique language (DSL) for modeling the database-to-code mapping. This proprietary syntax aims to express these models with minimum clutter (i.e., TypeScript generics and verbose code). Worried about having intellisense and validation? A well-crafted vscode extension gets you covered. In the following example, the prisma.schema file describes a DB with an Order table that has a one-to-many relation with a Country table:\\n\\n```prisma\\n// prisma.schema file\\nmodel Order {\\n  id                 Int      @id @default(autoincrement())\\n  userId             Int?\\n  paymentTermsInDays Int?\\n  deliveryAddress    String? @db.VarChar(255)\\n  country            Country  @relation(fields: [countryId], references: [id])\\n  countryId          Int\\n}\\n\\nmodel Country {\\n  id    Int     @id @default(autoincrement())\\n  name  String @db.VarChar(255)\\n  Order Order[]\\n}\\n```\\n\\n**B. Generate the client code -** Another unusual technique: to get the ORM code ready, one must invoke Prisma\'s CLI and ask for it: \\n\\n```bash\\nnpx prisma generate\\n```\\n\\nAlternatively, if you wish to have your DB ready and the code generated with one command, just fire:\\n\\n```bash\\nnpx prisma migrate deploy\\n```\\n\\nThis will generate migration files that you can execute later in production and also the ORM client code\\n\\n\\nThis will generate migration files that you can execute later in production and the TypeScript ORM code based on the model. The generated code location is defaulted under \'[root]/NODE_MODULES/.prisma/client\'. Every time the model changes, the code must get re-generated again. While most ORMs name this code \'repository\' or \'entity\' or \'active record\', interestingly, Prisma calls it a \'client\'. This shows part of its unique philosophy, which we will explore later\\n\\n**C. All good, use the client to interact with the DB -** The generated client has a rich set of functions and types for your DB interactions. Just import the ORM/client code and use it:\\n\\n```javascript\\nimport { PrismaClient } from \'.prisma/client\';\\n\\nconst prisma = new PrismaClient();\\n// A query example\\nawait prisma.order.findMany({\\n    where: {\\n      paymentTermsInDays: 30,\\n    },\\n    orderBy: {\\n      id: \'asc\',\\n    },\\n  });\\n// Use the same client for insertion, deletion, updates, etc\\n```\\n\\nThat\'s the nuts and bolts of Prisma. Is it different and better?\\n\\n## What is the same?\\n\\nWhen comparing options, before outlining differences, it\'s useful to state what is actually similar among these products. Here is a partial list of features that both TypeORM, Sequelize and Prisma support\\n\\n- Casual queries with sorting, filtering, distinct, group by, \'upsert\' (update or create),etc\\n- Raw queries\\n- Full text search\\n- Association/relations of any type (e.g., many to many, self-relation, etc)\\n- Aggregation queries\\n- Pagination\\n- CLI\\n- Transactions\\n- Migration & seeding\\n- Hooks/events (called middleware in Prisma)\\n- Connection pool\\n- Based on various community benchmarks, no dramatic performance differences\\n- All have huge amount of stars and downloads\\n\\nOverall, I found TypeORM and Sequelize to be a little more feature rich. For example, the following features are not supported only in Prisma: GIS queries, DB-level custom constraints, DB replication, soft delete, caching, exclude queries and some more\\n\\nWith that, shall we focus on what really set them apart and make a difference\\n\\n## What is fundamentally different?\\n\\n### 1. Type safety across the board\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** ORM\'s life is not easier since the TypeScript rise, to say the least. The need to support typed models/queries/etc yields a lot of developers sweat. Sequelize, for example, struggles to stabilize a TypeScript interface and, by now offers 3 different syntaxes + one external library ([sequelize-typescript](https://github.com/sequelize/sequelize-typescript)) that offers yet another style. Look at the syntax below, this feels like an afterthought - a library that was not built for TypeScript and now tries to squeeze it in somehow. Despite the major investment, both Sequelize and TypeORM offer only partial type safety. Simple queries do return typed objects, but other common corner cases like attributes/projections leave you with brittle strings. Here are a few examples:\\n\\n\\n```javascript\\n// Sequelize pesky TypeScript interface\\ntype OrderAttributes = {\\n  id: number,\\n  price: number,\\n  // other attributes...\\n};\\n\\ntype OrderCreationAttributes = Optional<OrderAttributes, \'id\'>;\\n\\n//\ud83d\ude2f Isn\'t this a weird syntax?\\nclass Order extends Model<InferAttributes<Order>, InferCreationAttributes<Order>> {\\n  declare id: CreationOptional<number>;\\n  declare price: number;\\n}\\n```\\n\\n```javascript\\n// Sequelize loose query types\\nawait getOrderModel().findAll({\\n    where: { noneExistingField: \'noneExistingValue\' } //\ud83d\udc4d TypeScript will warn here\\n    attributes: [\'none-existing-field\', \'another-imaginary-column\'], // No errors here although these columns do not exist\\n    include: \'no-such-table\', //\ud83d\ude2f no errors here although this table doesn\'t exist\\n  });\\n  await getCountryModel().findByPk(\'price\'); //\ud83d\ude2f No errors here although the price column is not a primary key\\n```\\n\\n```javascript\\n// TypeORM loose query\\nconst ordersOnSales: Post[] = await orderRepository.find({\\n  where: { onSale: true }, //\ud83d\udc4d TypeScript will warn here\\n  select: [\'id\', \'price\'],\\n})\\nconsole.log(ordersOnSales[0].userId); //\ud83d\ude2f No errors here although the \'userId\' column is not part of the returned object\\n```\\n\\nIsn\'t it ironic that a library called **Type**ORM base its queries on strings?\\n\\n\\n**\ud83e\udd14 How Prisma is different:** It takes a totally different approach by generating per-project client code that is fully typed. This client embodies types for everything: every query, relations, sub-queries, everything (except migrations). While other ORMs struggles to infer types from discrete models (including associations that are declared in other files), Prisma\'s offline code generation is easier: It can look through the entire DB relations, use custom generation code and build an almost perfect TypeScript experience. Why \'almost\' perfect? for some reason, Prisma advocates using plain SQL for migrations, which might result in a discrepancy between the code models and the DB schema. Other than that, this is how Prisma\'s client brings end to end type safety:\\n\\n```javascript\\nawait prisma.order.findMany({\\n    where: {\\n      noneExistingField: 1, //\ud83d\udc4d TypeScript error here\\n    },\\n    select: {\\n      noneExistingRelation: {  //\ud83d\udc4d TypeScript error here\\n        select: { id: true }, \\n      },\\n      noneExistingField: true,  //\ud83d\udc4d TypeScript error here\\n    },\\n  });\\n\\n  await prisma.order.findUnique({\\n    where: { price: 50 },  //\ud83d\udc4d TypeScript error here\\n  });\\n```\\n\\n**\ud83d\udcca How important:** TypeScript support across the board is valuable for DX mostly. Luckily, we have another safety net: The project testing. Since tests are mandatory, having build-time type verification is important but not a life saver\\n\\n![Medium importance](./medium2-importance-slider.png)\\n\\n**\ud83c\udfc6 Is Prisma doing better?:** Definitely\\n\\n## 2. Make you forget SQL\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** Many avoid ORMs while preferring to interact with the DB using lower-level techniques. One of their arguments is against the efficiency of ORMs: Since the generated queries are not visible immediately to the developers, wasteful queries might get executed unknowingly. While all ORMs provide syntactic sugar over SQL, there are subtle differences in the level of abstraction. The more the ORM syntax resembles SQL, the more likely the developers will understand their own actions\\n\\nFor example, TypeORM\'s query builder looks like SQL broken into convenient functions\\n\\n```javascript\\nawait createQueryBuilder(\'order\')\\n    .leftJoinAndSelect(\\n      \'order.userId\',\\n      \'order.productId\',\\n      \'country.name\',\\n      \'country.id\'\\n    )\\n    .getMany();\\n```\\n\\nA developer who read this code \ud83d\udc46 is likely to infer that a *join* query between two tables will get executed\\n\\n\\n**\ud83e\udd14 How Prisma is different:** Prisma\'s mission statement is to simplify DB work, the following statement is taken from their homepage:\\n\\n\\"We designed its API to be intuitive, both for SQL veterans and *developers brand new to databases*\\"\\n\\nBeing ambitious to appeal also to database layman, Prisma builds a syntax with a little bit higher abstraction, for example:\\n\\n```javascript\\nawait prisma.order.findMany({\\n    select: {\\n      userId: true,\\n      productId: true,\\n      country: {\\n        select: { name: true, id: true },\\n      },\\n    },\\n});\\n\\n```\\n\\nNo join is reminded here also it fetches records from two related tables (order, and country). Could you guess what SQL is being produced here? how many queries? One right, a simple join? Surprise,  actually, two queries are made. Prisma fires one query per-table here, as the join logic happens on the ORM client side (not inside the DB). But why?? in some cases, mostly where there is a lot of repetition in the DB cartesian join, querying each side of the relation is more efficient. But in other cases, it\'s not. Prisma arbitrarily chose what they believe will perform better in *most* cases. I checked, in my case it\'s *slower* than doing a one-join query on the DB side. As a developer, I would miss this deficiency due to the high-level syntax (no join is mentioned). My point is, Prisma sweet and simple syntax might be a  bless for developer who are brand new to databases and aim to achieve a working solution in a short time. For the longer term, having full awareness of the DB interactions is helpful, other ORMs encourage this awareness a little better\\n\\n**\ud83d\udcca How important:** Any ORM will hide SQL details from their users - without developer\'s awareness no ORM will save the day\\n\\n![Medium importance](./medium2-importance-slider.png)\\n\\n**\ud83c\udfc6 Is Prisma doing better?:** Not necessarily\\n\\n## 3. Performance\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** Speak to an ORM antagonist and you\'ll hear a common sensible argument: ORMs are much slower than a \'raw\' approach. To an extent, this is a legit observation as [most comparisons](https://welingtonfidelis.medium.com/pg-driver-vs-knex-js-vs-sequelize-vs-typeorm-f9ed53e9f802) will show none-negligible differences between raw/query-builder and ORM.\\n\\n![raw is faster d](./pg-driver-is-faster.png)\\n*Example: a direct insert against the PG driver is much shorter [Source](https://welingtonfidelis.medium.com/pg-driver-vs-knex-js-vs-sequelize-vs-typeorm-f9ed53e9f802)* \\n\\n It should also be noted that these benchmarks don\'t tell the entire story - on top of raw queries, every solution must build a mapper layer that maps the raw data to JS objects, nest the results, cast types, and more. This work is included within every ORM but not shown in benchmarks for the raw option. In reality, every team which doesn\'t use ORM would have to build their own small \\"ORM\\", including a mapper, which will also impact performance\\n\\n\\n**\ud83e\udd14 How Prisma is different:** It was my hope to see a magic here, eating the ORM cake without counting the calories, seeing Prisma achieving an almost \'raw\' query speed. I had some good and logical reasons for this hope: Prisma uses a DB client built with Rust. Theoretically, it could serialize to and nest objects faster (in reality, this happens on the JS side). It was also built from the ground up and could build on the knowledge pilled in ORM space for years. Also, since it returns POJOs only (see bullet \'No Active Record here!\') - no time should be spent on decorating objects with ORM fields\\n\\nYou already got it, this hope was not fulfilled. Going with every community benchmark ([one](https://dev.to/josethz00/benchmark-prisma-vs-typeorm-3873), [two](https://github.com/edgedb/imdbench), [three](https://deepkit.io/library)), Prisma at best is not faster than the average ORM. What is the reason? I can\'t tell exactly but it might be due the complicated system that must support Go, future languages, MongoDB and other non-relational DBs\\n\\n![Prisma is not faster](./throughput-benchmark.png)\\n*Example: Prisma is not faster than others. It should be noted that in other benchmarks Prisma scores higher and shows an \'average\' performance [Source](https://github.com/edgedb/imdbench)*\\n\\n**\ud83d\udcca How important:** It\'s expected from ORM users to live peacefully with inferior performance, for many systems it won\'t make a great deal. With that, 10%-30% performance differences between various ORMs are not a key factor\\n\\n![Medium importance](./medium2-importance-slider.png)\\n\\n**\ud83c\udfc6 Is Prisma doing better?:** No\\n\\n## 4. No active records here!\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** Node in its early days was heavily inspired by Ruby (e.g., testing \\"describe\\"), many great patterns were embraced, [Active Record](https://en.wikipedia.org/wiki/Active_record_pattern) is not among the successful ones. What is this pattern about in a nutshell? say you deal with Orders in your system, with Active Record an Order object/class will hold both the entity properties, possible also some of the logic functions and also CRUD functions. Many find this pattern to be awful, why? ideally, when coding some logic/flow, one should not keep her mind busy with side effects and DB narratives. It also might be that accessing some property unconsciously invokes a heavy DB call (i.e., lazy loading). If not enough, in case of heavy logic, unit tests might be in order (i.e., read [\'selective unit tests\'](https://blog.stevensanderson.com/2009/11/04/selective-unit-testing-costs-and-benefits/)) - it\'s going to be much harder to write unit tests against code that interacts with the DB. In fact, all of the respectable and popular architecture (e.g., DDD, clean, 3-tiers, etc) advocate to \'isolate the domain\', separate the core/logic of the system from the surrounding technologies. With all of that said, both TypeORM and Sequelize support the Active Record pattern which is displayed in many examples within their documentation. Both also support other better patterns like the data mapper (see below), but they still open the door for doubtful patterns\\n\\n\\n```javascript\\n// TypeORM active records \ud83d\ude1f\\n\\n@Entity()\\nclass Order extends BaseEntity {\\n    @PrimaryGeneratedColumn()\\n    id: number\\n\\n    @Column()\\n    price: number\\n\\n    @ManyToOne(() => Product, (product) => product.order)\\n    products: Product[]\\n\\n    // Other columns here\\n}\\n\\nfunction updateOrder(orderToUpdate: Order){\\n  if(orderToUpdate.price > 100){\\n    // some logic here\\n    orderToUpdate.status = \\"approval\\";\\n    orderToUpdate.save(); \\n    orderToUpdate.products.forEach((products) =>{ \\n\\n    })\\n    orderToUpdate.usedConnection = ? \\n  }\\n}\\n\\n\\n\\n```\\n\\n**\ud83e\udd14 How Prisma is different:** The better alternative is the data mapper pattern. It acts as a bridge, an adapter, between simple object notations (domain objects with properties) to the DB language, typically SQL. Call it with a plain JS object, POJO, get it saved in the DB. Simple. It won\'t add functions to the result objects or do anything beyond returning pure data, no surprising side effects. In its purest sense, this is a DB-related utility and completely detached from the business logic. While both Sequelize and TypeORM support this, Prisma offers *only* this style - no room for mistakes.\\n\\n\\n```javascript\\n// Prisma approach with a data mapper  \ud83d\udc4d\\n\\n// This was generated automatically by Prisma\\ntype Order {\\n    id: number\\n\\n    price: number\\n\\n    products: Product[]\\n\\n    // Other columns here\\n}\\n\\nfunction updateOrder(orderToUpdate: Order){\\n  if(orderToUpdate.price > 100){\\n    orderToUpdate.status = \\"approval\\";\\n    prisma.order.update({ where: { id: orderToUpdate.id }, data: orderToUpdate }); \\n    // Side effect \ud83d\udc46, but an explicit one. The thoughtful coder will move this to another function. Since it\'s happening outside, mocking is possible \ud83d\udc4d\\n    products.forEach((products) =>{ // No lazy loading, the data is already here \ud83d\udc4d\\n\\n    })\\n  }  \\n}\\n```\\n\\n In [Practica.js](https://github.com/practicajs/practica) we take it one step further and put the prisma models within the \\"DAL\\" layer and wrap it with the [repository pattern](https://learn.microsoft.com/en-us/dotnet/architecture/microservices/microservice-ddd-cqrs-patterns/infrastructure-persistence-layer-design). You may glimpse [into the code here](https://github.com/practicajs/practica/blob/21ff12ba19cceed9a3735c09d48184b5beb5c410/src/code-templates/services/order-service/domain/new-order-use-case.ts#L21), this is the business flow that calls the DAL layer\\n\\n\\n**\ud83d\udcca How important:** On the one hand, this is a key architectural principle to follow but the other hand most ORMs *allow* doing it right\\n\\n![Medium importance](./high1-importance-slider.png)\\n\\n**\ud83c\udfc6 Is Prisma doing better?:** Yes!\\n\\n## 5. Documentation and developer-experience\\n\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** TypeORM and Sequelize documentation is mediocre, though TypeORM is a little better. Based on my personal experience they do get a little better over the years, but still by no mean they deserve to be called \\"good\\" or \\"great\\". For example, if you seek to learn about \'raw queries\' - Sequelize offers [a very short page](https://sequelize.org/docs/v6/core-concepts/raw-queries/) on this matter, TypeORM info is spread in multiple other pages. Looking to learn about pagination? Couldn\'t find Sequelize documents, TypeORM has [some short explanation](https://typeorm.io/select-query-builder#using-pagination), 150 words only\\n\\n\\n**\ud83e\udd14 How Prisma is different:** Prisma documentation rocks! See their documents on similar topics: [raw queries](https://www.prisma.io/docs/concepts/components/prisma-client/raw-database-access) and [pagingation](https://www.prisma.io/docs/concepts/components/prisma-client/pagination), thousands of words, and dozens of code examples. The writing itself is also great, feels like some professional writers were involved\\n\\n![Prisma docs are comprehensive](./count-docs.png)\\n \\nThis chart above shows how comprehensive are Prisma docs (Obviously this by itself doesn\'t prove quality)\\n\\n**\ud83d\udcca How important:** Great docs are a key to awareness and avoiding pitfalls\\n\\n![Medium importance](./high1-importance-slider.png)\\n\\n\\n**\ud83c\udfc6 Is Prisma doing better?:** You bet\\n\\n## 6. Observability, metrics, and tracing\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** Good chances are (say about 99.9%) that you\'ll find yourself diagnostic slow queries in production or any other DB-related quirks. What can you expect from traditional ORMs in terms of observability? Mostly logging. [Sequelize provides both logging](https://sequelize.org/api/v7/interfaces/queryoptions#benchmark) of query duration and programmatic access to the connection pool state ({size,available,using,waiting}). [TypeORM provides only logging](https://orkhan.gitbook.io/typeorm/docs/logging) of queries that suppress a pre-defined duration threshold. This is better than nothing, but assuming you don\'t read production logs 24/7, you\'d probably need more than logging - an alert to fire when things seem faulty. To achieve this, it\'s your responsibility to bridge this info to your preferred monitoring system. Another logging downside for this sake is verbosity - we need to emit tons of information to the logs when all we really care for is the average duration. Metrics can serve this purpose much better as we\'re about to see soon with Prisma\\n\\nWhat if you need to dig into which specific part of the query is slow? unfortunately, there is no breakdown of the query phases duration - it\'s being left to you as a black-box\\n\\n```javascript\\n// Sequelize - logging various DB information\\n\\n```\\n\\n![Logging query duration](./sequelize-log.png)\\nLogging each query in order to realize trends and anomaly in the monitoring system\\n\\n\\n**\ud83e\udd14 How Prisma is different:** Since Prisma targets also enterprises, it must bring strong ops capabilities. Beautifully, it packs support for both [metrics](https://www.prisma.io/docs/concepts/components/prisma-client/metrics) and [open telemetry tracing](https://www.prisma.io/docs/concepts/components/prisma-client/opentelemetry-tracing)!. For metrics, it generates custom JSON with metric keys and values so anyone can adapt this to any monitoring system (e.g., CloudWatch, statsD, etc). On top of this, it produces out of the box metrics in [Prometheus](https://prometheus.io/) format (one of the most popular monitoring platforms). For example, the metric \'prisma_client_queries_duration_histogram_ms\' provides the average query length in the system overtime. What is even more impressive is the support for open-tracing - it feeds your OpenTelemetry collector with spans that describe the various phases of every query. For example, it might help realize what is the bottleneck in the query pipeline: Is it the DB connection, the query itself or the serialization?\\n\\n![prisma tracing](./trace-diagram.png)\\nPrisma visualizes the various query phases duration with open-telemtry \\n\\n**\ud83c\udfc6 Is Prisma doing better?:** Definitely\\n\\n\\n**\ud83d\udcca How important:** Goes without words how impactful is observability, however filling the gap in other ORM will demand no more than a few days\\n\\n![Medium importance](./medium2-importance-slider.png)\\n\\n## 7. Continuity - will it be here with us in 2024/2025\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** We live quite peacefully with the risk of one of our dependencies to disappear. With ORM though, this risk demand special attention because our buy-in is higher (i.e., harder to replace) and maintaining it was proven to be harder. Just look at a handful of successful ORMs in the past: objection.js, waterline, bookshelf - all of these respectful project had 0 commits in the past month. The single maintainer of objection.js [announced that he won\'t work the project anymore](https://github.com/Vincit/objection.js/issues/2335). This high churn rate is not surprising given the huge amount of moving parts to maintain, the gazillion corner cases and the modest \'budget\' OSS projects live with. Looking at OpenCollective shows that [Sequelize](https://opencollective.com/sequelize#category-BUDGET) and [TypeORM](https://opencollective.com/typeorm) are funded with ~1500$ month in average. This is barely enough to cover a daily Starbucks cappuccino and croissant (6.95$ x 365) for 5 maintainers. Nothing contrasts this model more than a startup company that just raised its series B - Prisma is [funded with 40,000,000$ (40 millions)](https://www.prisma.io/blog/series-b-announcement-v8t12ksi6x#:~:text=We%20are%20excited%20to%20announce,teams%20%26%20organizations%20in%20this%20article.) and recruited 80 people! Should not this inspire us with high confidence about their continuity? I\'ll surprisingly suggest that quite the opposite is true\\n\\nSee, an OSS ORM has to go over one huge hump, but a startup company must pass through TWO. The OSS project will struggle to achieve the critical mass of features, including some high technical barriers (e.g., TypeScript support, ESM). This typically lasts years, but once it does - a project can focus mostly on maintenance and step out of the danger zone. The good news for TypeORM and Sequelize is that they already did! Both struggled to keep their heads above the water, there were rumors in the past that [TypeORM is not maintained anymore](https://github.com/typeorm/typeorm/issues/3267), but they managed to go through this hump. I counted, both projects had approximately ~2000 PRs in the past 3 years! Going with [repo-tracker](https://repo-tracker.com/r/gh/sequelize/sequelize), each see multiple commits every week. They both have vibrant traction, and the majority of features you would expect from an ORM. TypeORM even supports beyond-the-basics features like multi data source and caching. It\'s unlikely that now, once they reached the promise land - they will fade away. It might happen, there is no guarantee in the OSS galaxy, but the risk is low\\n\\n![One hump](./one-hump.png)\\n\\n\\n**\ud83e\udd14 How Prisma is different:** Prisma a little lags behind in terms of features, but with a budget of 40M$ - there are good reasons to believe that they will pass the first hump, achieving a critical mass of features. I\'m more concerned with the second hump - showing revenues in 2 years or saying goodbye. As a company that is backed by venture capitals - the model is clear and cruel: In order to secure their next round, series B or C (depends whether the seed is counted), there must be a viable and proven business model. How do you \'sell\' ORM? Prisma experiments with multiple products, none is mature yet or being paid for. How big is this risk? According to [this startup companies success statistics](https://spdload.com/blog/startup-success-rate/), \\"About 65% of the Series A startups get series B, while 35% of the companies that get series A fail.\\". Since Prisma already gained a lot of love and adoption from the community, there success chances are higher than the average round A/B company, but even 20% or 10% chances to fade away is concerning\\n\\n> This is terrifying news - companies happily choose a young commercial OSS product without realizing that there are 10-30% chances for this product to disappear\\n\\n\\n![Two humps](./two-humps.png)\\n\\nSome of startup companies who seek a viable business model do not shut the doors rather change the product, the license or the free features. This is not my subjective business analysis, here are few examples: [MongoDB changed their license](https://techcrunch.com/2018/10/16/mongodb-switches-up-its-open-source-license/), this is why the majority had to host their Mongo DB over a single vendor. [Redis did something similar](https://techcrunch.com/2019/02/21/redis-labs-changes-its-open-source-license-again/). What are the chances of Prisma pivoting to another type of product? It actually already happened before, Prisma 1 was mostly about graphQL client and server, [it\'s now retired](https://github.com/prisma/prisma1)\\n\\nIt\'s just fair to mention the other potential path - most round B companies do succeed to qualify for the next round, when this happens even bigger money will be involved in building the \'Ferrari\' of JavaScript ORMs. I\'m surely crossing my fingers for these great people, at the same time we have to be conscious about our choices\\n\\n**\ud83d\udcca How important:** As important as having to code again the entire DB layer in a big system\\n\\n![Medium importance](./high2-importance-slider.png)\\n\\n\\n**\ud83c\udfc6 Is Prisma doing better?:** Quite the opposite\\n\\n## Closing - what should you use now?\\n\\nBefore proposing my key take away - which is the primary ORM, let\'s repeat the key learning that were introduced here:\\n\\n1. \ud83e\udd47 Prisma deserves a medal for its awesome DX, documentation, observability support and end-to-end TypeScript coverage\\n2. \ud83e\udd14 There are reasons to be concerned about Prisma\'s business continuity as a young startup without a viable business model. Also Prisma\'s abstract client syntax might blind developers a little more than other ORMs\\n3. \ud83c\udfa9 The contenders, TypeORM and Sequelize, matured and doing quite well: both have merged thousand PRs in the past 3 years to become more stable, they keep introducing new releases (see [repo-tracker](https://repo-tracker.com/r/gh/sequelize/sequelize)), and for now holds more features than Prisma. Also, both show solid performance (for an ORM). Hats off to the maintainers!\\n\\nBased on these observations, which should you pick? which ORM will we use for [practica.js](https://github.com/practicajs/practica)?\\n   \\nPrisma is an excellent addition to Node.js ORMs family, but not the hassle-free one tool to rule them all. It\'s a mixed bag of many delicious candies and a few gotchas. Wouldn\'t it grow to tick all the boxes? Maybe, but unlikely. Once built, it\'s too hard to dramatically change the syntax and engine performance. Then, during the writing and speaking with the community, including some Prisma enthusiasts, I realized that it doesn\'t aim to be the can-do-everything \'Ferrari\'. Its positioning seems to resemble more a convenient family car with a solid engine and awesome user experience. In other words, it probably aims for the enterprise space where there is mostly demand for great DX, OK performance, and business-class support\\n\\nIn the end of this journey I see no dominant flawless \'Ferrari\' ORM. I should probably change my perspective: Building ORM for the hectic modern JavaScript ecosystem is 10x harder than building a Java ORM back then in 2001. There is no stain in the shirt, it\'s a cool JavaScript swag. I learned to accept what we have, a rich set of features, tolerable performance, good enough for many systems. Need more? Don\'t use ORM. Nothing is going to change dramatically, it\'s now as good as it can be\\n\\n### When will it shine?\\n\\n**Surely use Prisma under these scenarios -** If your data needs are rather simple; when time-to-market concern takes precedence over the  data processing accuracy; when the DB is relatively small; if you\'re a mobile/frontend developer who is doing her first steps in the backend world; when there is a need for business-class support; AND when Prisma\'s long term business continuity risk is a non-issue for you\\n\\n**I\'d probably prefer other options under these conditions -** If the DB layer performance is a major concern; if you\'re savvy backend developer with solid SQL capabilities; when there is a need for fine grain control over the data layer. For all of these cases, Prisma might still work, but my primary choices would be using knex/TypeORM/Sequelize with a data-mapper style\\n\\nConsequently, we love Prisma and add it behind flag (--orm=prisma) to Practica.js. At the same time, until some clouds will disappear, Sequelize will remain our default ORM\\n\\n## Some of my other articles\\n\\n- [Book: Node.js testing best practices](https://github.com/testjavascript/nodejs-integration-tests-best-practices)\\n- [Book: JavaScript testing best practices](https://github.com/testjavascript/nodejs-integration-tests-best-practices)\\n- [Popular Node.js patterns and tools to re-consider](https://practica.dev/blog/popular-nodejs-pattern-and-tools-to-reconsider)\\n- [Practica.js - A Node.js starter](https://github.com/practicajs/practica)\\n- [Node.js best practices](https://github.com/goldbergyoni/nodebestpractices)"},{"id":"monorepo-backend","metadata":{"permalink":"/blog/monorepo-backend","editUrl":"https://github.com/practicajs/practica/tree/main/docs/blog/which-monorepo/index.md","source":"@site/blog/which-monorepo/index.md","title":"Which Monorepo is right for a Node.js BACKEND\xa0now?","description":"As a Node.js starter, choosing the right libraries and frameworks for our users is the bread and butter of our work in Practica.js. In this post, we\'d like to share our considerations in choosing our monorepo tooling","date":"2022-11-07T11:00:00.000Z","formattedDate":"November 7, 2022","tags":[{"label":"monorepo","permalink":"/blog/tags/monorepo"},{"label":"decisions","permalink":"/blog/tags/decisions"}],"readingTime":16.925,"hasTruncateMarker":true,"authors":[{"name":"Yoni Goldberg","title":"Practica.js core maintainer","url":"https://github.com/goldbergyoni","imageURL":"https://github.com/goldbergyoni.png","key":"goldbergyoni"},{"name":"Michael Salomon","title":"Practica.js core maintainer","url":"https://github.com/mikicho","imageURL":"https://avatars.githubusercontent.com/u/11459632?v=4","key":"michaelsalomon"}],"frontMatter":{"slug":"monorepo-backend","date":"2022-11-07T11:00","title":"Which Monorepo is right for a Node.js BACKEND\xa0now?","authors":["goldbergyoni","michaelsalomon"],"tags":["monorepo","decisions"]},"prevItem":{"title":"Is Prisma better than your \'traditional\' ORM?","permalink":"/blog/is-prisma-better-than-your-traditional-orm"},"nextItem":{"title":"Popular Node.js patterns and tools to re-consider","permalink":"/blog/popular-nodejs-pattern-and-tools-to-reconsider"}},"content":"As a Node.js starter, choosing the right libraries and frameworks for our users is the bread and butter of our work in [Practica.js](https://github.com/practicajs/practica). In this post, we\'d like to share our considerations in choosing our monorepo tooling\\n\\n![Monorepos](./monorepo-high-level.png)\\n\\n## What are we looking\xa0at\\n\\n\\nThe Monorepo market is hot like fire. Weirdly, now when the demand for Monoreps is exploding, one of the leading libraries \u2014  [Lerna- has just retired.](https://github.com/lerna/lerna/issues/2703)  When looking closely, it might not be just a coincidence \u2014 With so many disruptive and shiny features brought on by new vendors, Lerna failed to keep up with the pace and stay relevant. This bloom of new tooling gets many confused \u2014 What is the right choice for my next project? What should I look at when choosing a Monorepo tool? This post is all about curating this information overload, covering the new tooling, emphasizing what is important, and finally share some recommendations. If you are here for tools and features, you\u2019re in the right place, although you might find yourself on a soul-searching journey to what is your desired development workflow.\\n\\nThis post is concerned with backend-only and Node.js. It also scoped to  _typical_  business solutions. If you\u2019re Google/FB developer who is faced with 8,000 packages \u2014 sorry, you need special gear. Consequently, monster Monorepo tooling like  [Bazel](https://github.com/thundergolfer/example-bazel-monorepo)  is left-out. We will cover here some of the most popular Monorepo tools including Turborepo, Nx, PNPM, Yarn/npm workspace, and Lerna (although it\u2019s not actually maintained anymore \u2014 it\u2019s a good baseline for comparison).\\n\\nLet\u2019s start? When human beings use the term Monorepo, they typically refer to one or more of the following  _4 layers below._ Each one of them can bring value to your project, each has different consequences, tooling, and features:\\n\\n\x3c!--truncate--\x3e\\n\\n\\n# Layer 1: Plain old folders to stay on top of your code\\n\\nWith zero tooling and only by having all the Microservice and libraries together in the same root folder, a developer gets great management perks and tons of value: Navigation, search across components, deleting a library instantly, debugging,  _quickly_  adding new components. Consider the alternative with multi-repo approach \u2014 adding a new component for modularity demands opening and configuring a new  GitHub  repository. Not just a hassle but also greater chances of developers choosing the short path and including the new code in some semi-relevant existing package. In plain words, zero-tooling Monorepos can increase modularity.\\n\\nThis layer is often overlooked. If your codebase is not huge and the components are highly decoupled (more on this later)\u2014 it might be all you need. We\u2019ve seen a handful of successful Monorepo solutions without any special tooling.\\n\\nWith that said, some of the newer tools augment this experience with interesting features:\\n\\n-   Both  [Turborepo](https://turborepo.org/)  and  [Nx](https://nx.dev/structure/dependency-graph)  and also  [Lerna](https://www.npmjs.com/package/lerna-dependency-graph)  provide a visual representation of the packages\u2019 dependencies\\n-   [Nx allows \u2018visibility rules\u2019](https://nx.dev/structure/monorepo-tags)  which is about enforcing who can use what. Consider, a \u2018checkout\u2019 library that should be approached only by the \u2018order Microservice\u2019 \u2014 deviating from this will result in failure during development (not runtime enforcement)\\n\\n![](https://miro.medium.com/max/1400/0*pHZKRlGT6iOKCmzg.jpg)\\n\\nNx dependencies graph\\n\\n-   [Nx workspace generator](https://nx.dev/generators/workspace-generators)  allows scaffolding out components. Whenever a team member needs to craft a new controller/library/class/Microservice, she just invokes a CLI command which products code based on a community or organization template. This enforces consistency and best practices sharing\\n\\n# Layer 2: Tasks and pipeline to build your code efficiently\\n\\nEven in a world of autonomous components, there are management tasks that must be applied in a batch like applying a security patch via npm update, running the tests of  _multiple_  components that were affected by a change, publish 3 related libraries to name a few examples. All Monorepo tools support this basic functionality of invoking some command over a group of packages. For example, Lerna, Nx, and Turborepo do.\\n\\n![](https://miro.medium.com/max/1400/1*wu7xtN97-Ihz4uCSDwd0mA.png)\\n\\nApply some commands over multiple packages\\n\\nIn some projects, invoking a cascading command is all you need. Mostly if each package has an autonomous life cycle and the build process spans a single package (more on this later). In some other types of projects where the workflow demands testing/running and publishing/deploying many packages together \u2014 this will end in a terribly slow experience. Consider a solution with hundred of packages that are transpiled and bundled \u2014 one might wait minutes for a wide test to run. While it\u2019s not always a great practice to rely on wide/E2E tests, it\u2019s quite common in the wild. This is exactly where the new wave of Monorepo tooling shines \u2014  _deeply_  optimizing the build process. I should say this out loud: These tools bring beautiful and innovative build optimizations:\\n\\n-   **Parallelization \u2014** If two commands or packages are orthogonal to each other, the commands will run in two different threads or processes. Typically your quality control involves testing, lining, license checking, CVE checking \u2014 why not parallelize?\\n-   **Smart execution plan \u2014**Beyond parallelization, the optimized tasks execution order is determined based on many factors. Consider a build that includes A, B, C where A, C depend on B \u2014 naively, a build system would wait for B to build and only then run A & C. This can be optimized if we run A & C\u2019s  _isolated_  unit tests  _while_ building B and not afterward. By running task in parallel as early as possible, the overall execution time is improved \u2014 this has a remarkable impact mostly when hosting a high number of components. See below a visualization example of a pipeline improvement\\n\\n![](https://miro.medium.com/max/1400/0*C6cxCblQU8ckTIQk.png)\\n\\nA modern tool advantage over old Lerna. Taken from Turborepo website\\n\\n-   **Detect who is affected by a change \u2014**  Even on a system with high coupling between packages, it\u2019s usually not necessary to run  _all_ packages rather than only those who are affected by a change. What exactly is \u2018affected\u2019? Packages/Microservices that depend upon another package that has changed. Some of the toolings can ignore minor changes that are unlikely to break others. This is not a great performance booster but also an amazing testing feature \u2014developers can get quick feedback on whether any of their clients were broken. Both Nx and Turborepo support this feature. Lerna can tell only which of the Monorepo package has changed\\n-   **Sub-systems (i.e., projects) \u2014** Similarly to \u2018affected\u2019 above, modern tooling can realize portions of the graph that are inter-connected (a project or application) while others are not reachable by the component in context (another project) so they know to involve only packages of the relevant group\\n-   **Caching \u2014** This is a serious speed booster: Nx and Turborepo cache the result/output of tasks and avoid running them again on consequent builds if unnecessary. For example, consider long-running tests of a Microservice, when commanding to re-build this Microservice, the tooling might realize that nothing has changed and the test will get skipped. This is achieved by generating a hashmap of all the dependent resources \u2014 if any of these resources haven\u2019t change, then the hashmap will be the same and the task will get skipped. They even cache the stdout of the command, so when you run a cached version it acts like the real thing \u2014 consider running 200 tests, seeing all the log statements of the tests, getting results over the terminal in 200 ms, everything acts like \u2018real testing while in fact, the tests did not run at all rather the cache!\\n-   **Remote caching \u2014** Similarly to caching, only by placing the task\u2019s hashmaps and result on a global server so further executions on other team member\u2019s computers will also skip unnecessary tasks. In huge Monorepo projects that rely on E2E tests and must build all packages for development, this can save a great deal of time\\n\\n# Layer 3: Hoist your dependencies to boost npm installation\\n\\nThe speed optimizations that were described above won\u2019t be of help if the bottleneck is the big bull of mud that is called \u2018npm install\u2019 (not to criticize, it\u2019s just hard by nature). Take a typical scenario as an example, given dozens of components that should be built, they could easily trigger the installation of thousands of sub-dependencies. Although they use quite similar dependencies (e.g., same logger, same ORM), if the dependency version is not equal then npm will duplicate ([the NPM doppelgangers problem](https://rushjs.io/pages/advanced/npm_doppelgangers/)) the installation of those packages which might result in a long process.\\n\\nThis is where the workspace line of tools (e.g., Yarn workspace, npm workspaces, PNPM) kicks in and introduces some optimization \u2014 Instead of installing dependencies inside each component \u2018NODE_MODULES\u2019 folder, it will create one centralized folder and link all the dependencies over there. This can show a tremendous boost in install time for huge projects. On the other hand, if you always focus on one component at a time, installing the packages of a single Microservice/library should not be a concern.\\n\\nBoth Nx and Turborepo can rely on the package manager/workspace to provide this layer of optimizations. In other words, Nx and Turborepo are the layer above the package manager who take care of optimized dependencies installation.\\n\\n![](https://miro.medium.com/max/1400/1*dhyCWSbzpIi5iagR4OB4zQ.png)\\n\\nOn top of this, Nx introduces one more non-standard, maybe even controversial, technique: There might be only ONE package.json at the root folder of the entire Monorepo. By default, when creating components using Nx, they will not have their own package.json! Instead, all will share the root package.json.  Going this way, all the Microservice/libraries share their dependencies and the installation time is improved.  Note: It\u2019s possible to create \u2018publishable\u2019 components that do have a package.json, it\u2019s just not the default.\\n\\nI\u2019m concerned here. Sharing dependencies among packages increases the coupling, what if Microservice1 wishes to bump dependency1 version but Microservice2 can\u2019t do this at the moment? Also, package.json is part of Node.js  _runtime_ and excluding it from the component root loses important features like package.json main field or ESM exports (telling the clients which files are exposed). I ran some POC with Nx last week and found myself blocked \u2014 library B was wadded, I tried to import it from Library A but couldn\u2019t get the \u2018import\u2019 statement to specify the right package name. The natural action was to open B\u2019s package.json and check the name, but there is no Package.json\u2026 How do I determine its name? Nx docs are great, finally, I found the answer, but I had to spend time learning a new \u2018framework\u2019.\\n\\n# Stop for a second: It\u2019s all about your workflow\\n\\nWe deal with tooling and features, but it\u2019s actually meaningless evaluating these options before determining whether your preferred workflow is  _synchronized or independent_ (we will discuss this in a few seconds)_._ This upfront  _fundamental_  decision will change almost everything.\\n\\nConsider the following example with 3 components: Library 1 is introducing some major and breaking changes, Microservice1 and Microservice2 depend upon Library1 and should react to those breaking changes. How?\\n\\n**Option A \u2014 The synchronized workflow-** Going with this development style, all the three components will be developed and deployed in one chunk  _together_. Practically, a developer will code the changes in Library1, test libray1 and also run wide integration/e2e tests that include Microservice1 and Microservice2. When they\'re ready, the version of all components will get bumped. Finally, they will get deployed  _together._\\n\\nGoing with this approach, the developer has the chance of seeing the full flow from the client\'s perspective (Microservice1 and 2), the tests cover not only the library but also through the eyes of the clients who actually use it. On the flip side, it mandates updating all the depend-upon components (could be dozens), doing so increases the risk\u2019s blast radius as more units are affected and should be considered before deployment. Also, working on a large unit of work demands building and testing more things which will slow the build.\\n\\n**Option B \u2014 Independent workflow-** This style is about working a unit by unit, one bite at a time, and deploy each component independently based on its personal business considerations and priority. This is how it goes: A developer makes the changes in Library1, they must be tested carefully in the scope of Library1. Once she is ready, the SemVer is bumped to a new major and the library is published to a package manager registry (e.g., npm). What about the client Microservices? Well, the team of Microservice2 is super-busy now with other priorities, and skip this update for now (the same thing as we all delay many of our npm updates,). However, Microservice1 is very much interested in this change \u2014 The team has to pro-actively update this dependency and grab the latest changes, run the tests and when they are ready, today or next week \u2014 deploy it.\\n\\nGoing with the independent workflow, the library author can move much faster because she does not need to take into account 2 or 30 other components \u2014 some are coded by different teams. This workflow also  _forces her_  to write efficient tests against the library \u2014 it\u2019s her only safety net and is likely to end with autonomous components that have low coupling to others. On the other hand, testing in isolation without the client\u2019s perspective loses some dimension of realism. Also, if a single developer has to update 5 units \u2014 publishing each individually to the registry and then updating within all the dependencies can be a little tedious.\\n\\n![](https://miro.medium.com/max/1400/1*eeJFL3_vo5tCrWvVY-surg.png)\\n\\nSynchronized and independent workflows illustrated\\n\\n**On the illusion of synchronicity**\\n\\nIn distributed systems, it\u2019s not feasible to achieve 100% synchronicity \u2014 believing otherwise can lead to design faults. Consider a breaking change in Microservice1, now its client Microservice2 is adapting and ready for the change. These two Microservices are deployed together but due to the nature of Microservices and distributed runtime (e.g., Kubernetes) the deployment of Microservice1 only fail. Now, Microservice2\u2019s code is not aligned with Microservice1 production and we are faced with a production bug. This line of failures can be handled to an extent also with a synchronized workflow \u2014 The deployment should orchestrate the rollout of each unit so each one is deployed at a time. Although this approach is doable, it increased the chances of large-scoped rollback and increases deployment fear.\\n\\nThis fundamental decision, synchronized or independent, will determine so many things \u2014 Whether performance is an issue or not at all (when working on a single unit), hoisting dependencies or leaving a dedicated node_modules in every package\u2019s folder, and whether to create a local link between packages which is described in the next paragraph.\\n\\n# Layer 4: Link your packages for immediate feedback\\n\\nWhen having a Monorepo, there is always the unavoidable dilemma of how to link between the components:\\n\\n**Option 1: Using npm \u2014** Each library is a standard npm package and its client installs it via the standards npm commands. Given Microservice1 and Library1, this will end with two copies of Library1: the one inside Microservices1/NODE_MODULES (i.e., the local copy of the consuming Microservice), and the 2nd is the development folder where the team is coding Library1.\\n\\n**Option2: Just a plain folder \u2014** With this, Library1 is nothing but a logical module inside a folder that Microservice1,2,3 just locally imports. NPM is not involved here, it\u2019s just code in a dedicated folder. This is for example how Nest.js modules are represented.\\n\\nWith option 1, teams benefit from all the great merits of a package manager \u2014 SemVer(!), tooling, standards, etc. However, should one update Library1, the changes won\u2019t get reflected in Microservice1 since it is grabbing its copy from the npm registry and the changes were not published yet. This is a fundamental pain with Monorepo and package managers \u2014 one can\u2019t just code over multiple packages and test/run the changes.\\n\\nWith option 2, teams lose all the benefits of a package manager: Every change is propagated immediately to all of the consumers.\\n\\nHow do we bring the good from both worlds (presumably)? Using linking. Lerna, Nx, the various package manager workspaces (Yarn, npm, etc) allow using npm libraries and at the same time link between the clients (e.g., Microservice1) and the library. Under the hood, they created a symbolic link. In development mode, changes are propagated immediately, in deployment time \u2014 the copy is grabbed from the registry.\\n\\n![](https://miro.medium.com/max/1400/1*9PkNrnbnibFdbvPieq-y9g.png)\\n\\nLinking packages in a Monorepo\\n\\nIf you\u2019re doing the synchronized workflow, you\u2019re all set. Only now any risky change that is introduced by Library3, must be handled NOW by the 10 Microservices that consume it.\\n\\nIf favoring the independent workflow, this is of course a big concern. Some may call this direct linking style a \u2018monolith monorepo\u2019, or maybe a \u2018monolitho\u2019. However, when not linking, it\u2019s harder to debug a small issue between the Microservice and the npm library. What I typically do is  _temporarily link_ (with npm link) between the packages_,_  debug, code, then finally remove the link.\\n\\nNx is taking a slightly more disruptive approach \u2014 it is using  [TypeScript paths](https://www.typescriptlang.org/tsconfig#paths)  to bind between the components. When Microservice1 is importing Library1, to avoid the full local path, it creates a TypeScript mapping between the library name and the full path. But wait a minute, there is no TypeScript in production so how could it work? Well, in serving/bundling time it webpacks and stitches the components together. Not a very standard way of doing Node.js work.\\n\\n# Closing: What should you use?\\n\\nIt\u2019s all about your workflow and architecture \u2014 a huge unseen cross-road stands in front of the Monorepo tooling decision.\\n\\n**Scenario A \u2014** If your architecture dictates a  _synchronized workflow_  where all packages are deployed together, or at least developed in collaboration \u2014 then there is a strong need for a rich tool to manage this coupling and boost the performance. In this case, Nx might be a great choice.\\n\\nFor example, if your Microservice must keep the same versioning, or if the team really small and the same people are updating all the components, or if your modularization is not based on package manager but rather on framework-own modules (e.g., Nest.js), if you\u2019re doing frontend where the components inherently are published together, or if your testing strategy relies on E2E mostly \u2014 for all of these cases and others, Nx is a tool that was built to enhance the experience of coding many  _relatively_ coupled components together. It is a great a sugar coat over systems that are unavoidably big and linked.\\n\\nIf your system is not inherently big or meant to synchronize packages deployment, fancy Monorepo features might increase the coupling between components. The Monorepo pyramid above draws a line between basic features that provide value without coupling components while other layers come with an architectural price to consider. Sometimes climbing up toward the tip is worth the consequences, just make this decision consciously.\\n\\n![](https://miro.medium.com/max/1400/1*c2qYYpVGG667bkum-gB-5Q.png)\\n\\n**Scenario B\u2014** If you\u2019re into an  _independent workflow_ where each package is developed, tested, and deployed (almost) independently \u2014 then inherently there is no need to fancy tools to orchestrate hundreds of packages. Most of the time there is just one package in focus. This calls for picking a leaner and simpler tool \u2014 Turborepo. By going this route, Monorepo is not something that affects your architecture, but rather a scoped tool for faster build execution. One specific tool that encourages an independent workflow is  [Bilt](https://github.com/giltayar/bilt)  by Gil Tayar, it\u2019s yet to gain enough popularity but it might rise soon and is a great source to learn more about this philosophy of work.\\n\\n**In any scenario, consider workspaces \u2014** If you face performance issues that are caused by package installation, then the various workspace tools Yarn/npm/PNPM, can greatly minimize this overhead with a low footprint. That said, if you\u2019re working in an autonomous workflow, smaller are the chances of facing such issues. Don\u2019t just use tools unless there is a pain.\\n\\nWe tried to show the beauty of each and where it shines. If we\u2019re allowed to end this article with an opinionated choice: We greatly believe in an independent and autonomous workflow where the occasional developer of a package can code and deploy fearlessly without messing with dozens of other foreign packages. For this reason, Turborepo will be our favorite tool for the next season. We promise to tell you how it goes.\\n\\n# Bonus: Comparison table\\n\\nSee below a detailed comparison table of the various tools and features:\\n\\n![](https://miro.medium.com/max/1400/1*iHX_IdPW8XXXiZTyjFo6bw.png)\\n\\nPreview only, the complete table can be  [found here](https://github.com/practicajs/practica/blob/main/docs/docs/decisions/monorepo.md)"},{"id":"popular-nodejs-pattern-and-tools-to-reconsider","metadata":{"permalink":"/blog/popular-nodejs-pattern-and-tools-to-reconsider","editUrl":"https://github.com/practicajs/practica/tree/main/docs/blog/pattern-to-reconsider/index.md","source":"@site/blog/pattern-to-reconsider/index.md","title":"Popular Node.js patterns and tools to re-consider","description":"Node.js is maturing. Many patterns and frameworks were embraced - it\'s my belief that developers\' productivity dramatically increased in the past years. One downside of maturity is habits - we now reuse existing techniques more often. How is this a problem?","date":"2022-08-02T10:00:00.000Z","formattedDate":"August 2, 2022","tags":[{"label":"node.js","permalink":"/blog/tags/node-js"},{"label":"express","permalink":"/blog/tags/express"},{"label":"nestjs","permalink":"/blog/tags/nestjs"},{"label":"fastify","permalink":"/blog/tags/fastify"},{"label":"passport","permalink":"/blog/tags/passport"},{"label":"dotenv","permalink":"/blog/tags/dotenv"},{"label":"supertest","permalink":"/blog/tags/supertest"},{"label":"practica","permalink":"/blog/tags/practica"},{"label":"testing","permalink":"/blog/tags/testing"}],"readingTime":21.09,"hasTruncateMarker":true,"authors":[{"name":"Yoni Goldberg","title":"Practica.js core maintainer","url":"https://github.com/goldbergyoni","imageURL":"https://github.com/goldbergyoni.png","key":"goldbergyoni"}],"frontMatter":{"slug":"popular-nodejs-pattern-and-tools-to-reconsider","date":"2022-08-02T10:00","hide_table_of_contents":true,"title":"Popular Node.js patterns and tools to re-consider","authors":["goldbergyoni"],"tags":["node.js","express","nestjs","fastify","passport","dotenv","supertest","practica","testing"]},"prevItem":{"title":"Which Monorepo is right for a Node.js BACKEND\xa0now?","permalink":"/blog/monorepo-backend"},"nextItem":{"title":"Practica.js v0.0.1 is alive","permalink":"/blog/practica-is-alive"}},"content":"Node.js is maturing. Many patterns and frameworks were embraced - it\'s my belief that developers\' productivity dramatically increased in the past years. One downside of maturity is habits - we now reuse existing techniques more often. How is this a problem?\\n\\nIn his novel book \'Atomic Habits\' the author James Clear states that:\\n\\n> \\"Mastery is created by habits. However, sometimes when we\'re on auto-pilot performing habits, we tend to slip up... Just being we are gaining experience through performing the habits does not mean that we are improving. We actually go backwards on the improvement scale with most habits that turn into auto-pilot\\". In other words, practice makes perfect, and bad practices make things worst\\n\\nWe copy-paste mentally and physically things that we are used to, but these things are not necessarily right anymore. Like animals who shed their shells or skin to adapt to a new reality, so the Node.js community should constantly gauge its existing patterns, discuss and change\\n\\nLuckily, unlike other languages that are more committed to specific design paradigms (Java, Ruby) - Node is a house of many ideas. In this community, I feel safe to question some of our good-old tooling and patterns. The list below contains my personal beliefs, which are brought with reasoning and examples. \\n\\nAre those disruptive thoughts surely correct? I\'m not sure. There is one things I\'m sure about though - For Node.js to live longer, we need to encourage critics, focus our loyalty on innovation, and keep the discussion going. The outcome of this discussion is not \\"don\'t use this tool!\\" but rather becoming familiar with other techniques that, _under some circumstances_ might be a better fit\\n\\n![Animals and frameworks shed their skin](./crab.webp)\\n\\n_The True Crab\'s exoskeleton is hard and inflexible, he must shed his restrictive exoskeleton to grow and reveal the new roomier shell_\\n\\n\\n## TOC - Patterns to reconsider\\n\\n1. Dotenv\\n2. Calling a service from a controller\\n3. Nest.js dependency injection for all classes\\n4. Passport.js\\n5. Supertest\\n6. Fastify utility decoration\\n7. Logging from a catch clause\\n8. Morgan logger\\n9. NODE_ENV\\n\\n\x3c!--truncate--\x3e\\n## 1. Dotenv as your configuration source\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** A super popular technique in which the app configurable values (e.g., DB user name) are stored in a simple text file. Then, when the app loads, the dotenv library sets all the text file values as environment variables so the code can read this\\n\\n```javascript\\n// .env file\\nUSER_SERVICE_URL=https://users.myorg.com\\n\\n//start.js\\nrequire(\'dotenv\').config();\\n\\n//blog-post-service.js\\nrepository.savePost(post);\\n//update the user number of posts, read the users service URL from an environment variable\\nawait axios.put(`${process.env.USER_SERVICE_URL}/api/user/${post.userId}/incrementPosts`)\\n\\n```\\n\\n**\ud83d\udcca How popular:** 21,806,137 downloads/week!\\n\\n**\ud83e\udd14 Why it might be wrong:** Dotenv is so easy and intuitive to start with, so one might easily overlook fundamental features: For example, it\'s hard to infer the configuration schema and realize the meaning of each key and its typing. Consequently, there is no built-in way to fail fast when a mandatory key is missing - a flow might fail after starting and presenting some side effects (e.g., DB records were already mutated before the failure). In the example above, the blog post will be saved to DB, and only then will the code realize that a mandatory key is missing - This leaves the app hanging in an invalid state. On top of this, in the presence of many keys, it\'s impossible to organize them hierarchically. If not enough, it encourages developers to commit this .env file which might contain production values - this happens because there is no clear way to define development defaults. Teams usually work around this by committing .env.example file and then asking whoever pulls code to rename this file manually. If they remember to of course\\n\\n**\u2600\ufe0f Better alternative:** Some configuration libraries provide out of the box solution to all of these needs. They encourage a clear schema and the possibility to validate early and fail if needed. See [comparison of options here](https://practica.dev/decisions/configuration-library). One of the better alternatives is [\'convict\'](https://github.com/mozilla/node-convict), down below is the same example, this time with Convict, hopefully it\'s better now:\\n\\n```javascript\\n// config.js\\nexport default {\\n  userService: {\\n    url: {\\n      // Hierarchical, documented and strongly typed \ud83d\udc47\\n      doc: \\"The URL of the user management service including a trailing slash\\",\\n      format: \\"url\\",\\n      default: \\"http://localhost:4001\\",\\n      nullable: false,\\n      env: \\"USER_SERVICE_URL\\",\\n    },\\n  },\\n  //more keys here\\n};\\n\\n//start.js\\nimport convict from \\"convict\\";\\nimport configSchema from \\"config\\";\\nconvict(configSchema);\\n// Fail fast!\\nconvictConfigurationProvider.validate();\\n\\n//blog-post.js\\nrepository.savePost(post);\\n// Will never arrive here if the URL is not set\\nawait axios.put(\\n  `${convict.get(userService.url)}/api/user/${post.userId}/incrementPosts`\\n);\\n```\\n\\n## 2. Calling a \'fat\' service from the API controller\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** Consider a reader of our code who wishes to understand the entire _high-level_ flow or delve into a very _specific_ part. She first lands on the API controller, where requests start. Unlike what its name implies, this controller layer is just an adapter and kept really thin and straightforward. Great thus far. Then the controller calls a big \'service\' with thousands of lines of code that represent the entire logic\\n\\n```javascript\\n// user-controller\\nrouter.post(\'/\', async (req, res, next) => {\\n    await userService.add(req.body);\\n    // Might have here try-catch or error response logic\\n}\\n\\n// user-service\\nexports function add(newUser){\\n    // Want to understand quickly? Need to understand the entire user service, 1500 loc\\n    // It uses technical language and reuse narratives of other flows\\n    this.copyMoreFieldsToUser(newUser)\\n    const doesExist = this.updateIfAlreadyExists(newUser)\\n    if(!doesExist){\\n        addToCache(newUser);\\n    }\\n    // 20 more lines that demand navigating to other functions in order to get the intent\\n}\\n\\n\\n```\\n\\n**\ud83d\udcca How popular:** It\'s hard to pull solid numbers here, I could confidently say that in _most_ of the app that I see, this is the case\\n\\n**\ud83e\udd14 Why it might be wrong:** We\'re here to tame complexities. One of the useful techniques is deferring a complexity to the later stage possible. In this case though, the reader of the code (hopefully) starts her journey through the tests and the controller - things are simple in these areas. Then, as she lands on the big service - she gets tons of complexity and small details, although she is focused on understanding the overall flow or some specific logic. This is **unnecessary** complexity\\n\\n**\u2600\ufe0f Better alternative:** The controller should call a particular type of service, a **use-case** , which is responsible for _summarizing_ the flow in a business and simple language. Each flow/feature is described using a use-case, each contains 4-10 lines of code, that tell the story without technical details. It mostly orchestrates other small services, clients, and repositories that hold all the implementation details. With use cases, the reader can grasp the high-level flow easily. She can now **choose** where she would like to focus. She is now exposed only to **necessary** complexity. This technique also encourages partitioning the code to the smaller object that the use-case orchestrates. Bonus: By looking at coverage reports, one can tell which features are covered, not just files/functions\\n\\nThis idea by the way is formalized in the [\'clean architecture\' book](https://www.bookdepository.com/Clean-Architecture-Robert-Martin/9780134494166?redirected=true&utm_medium=Google&utm_campaign=Base1&utm_source=IL&utm_content=Clean-Architecture&selectCurrency=ILS&w=AFF9AU99ZB4MTDA8VTRQ&gclid=Cj0KCQjw3eeXBhD7ARIsAHjssr92kqLn60dnfQCLjbkaqttdgvhRV5dqKtnY680GCNDvKp-16HtZp24aAg6GEALw_wcB) - I\'m not a big fan of \'fancy\' architectures, but see - it\'s worth cherry-picking techniques from every source. You may walk-through our [Node.js best practices starter, practica.js](https://github.com/practicajs/practica), and examine the use-cases code\\n\\n```javascript\\n// add-order-use-case.js\\nexport async function addOrder(newOrder: addOrderDTO) {\\n  orderValidation.assertOrderIsValid(newOrder);\\n  const userWhoOrdered = await userServiceClient.getUserWhoOrdered(\\n    newOrder.userId\\n  );\\n  paymentTermsService.assertPaymentTerms(\\n    newOrder.paymentTermsInDays,\\n    userWhoOrdered.terms\\n  );\\n\\n  const response = await orderRepository.addOrder(newOrder);\\n\\n  return response;\\n}\\n```\\n\\n## 3. Nest.js: Wire _everything_ with dependency injection\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** If you\'re doing Nest.js, besides having a powerful framework in your hands, you probably use DI for _everything_ and make every class injectable. Say you have a weather-service that depends upon humidity-service, and **there is no requirement to swap** the humidity-service with alternative providers. Nevertheless, you inject humidity-service into the weather-service. It becomes part of your development style, \\"why not\\" you think - I may need to stub it during testing or replace it in the future\\n\\n```typescript\\n// humidity-service.ts - not customer facing\\n@Injectable()\\nexport class GoogleHumidityService {\\n\\n  async getHumidity(when: Datetime): Promise<number> {\\n    // Fetches from some specific cloud service\\n  }\\n}\\n\\n// weather-service.ts - customer facing\\nimport { GoogleHumidityService } from \'./humidity-service.ts\';\\n\\nexport type weatherInfo{\\n    temperature: number,\\n    humidity: number\\n}\\n\\nexport class WeatherService {\\n  constructor(private humidityService: GoogleHumidityService) {}\\n\\n  async GetWeather(when: Datetime): Promise<weatherInfo> {\\n    // Fetch temperature from somewhere and then humidity from GoogleHumidityService\\n  }\\n}\\n\\n// app.module.ts\\n@Module({\\n  providers: [GoogleHumidityService, WeatherService],\\n})\\nexport class AppModule {}\\n```\\n\\n**\ud83d\udcca How popular:** No numbers here but I could confidently say that in _all_ of the Nest.js app that I\'ve seen, this is the case. In the popular [\'nestjs-realworld-example-ap[p\'](](https://github.com/lujakob/nestjs-realworld-example-app)) all the services are \'injectable\'\\n\\n**\ud83e\udd14 Why it might be wrong:** Dependency injection is not a priceless coding style but a pattern you should pull in the right moment, like any other pattern. Why? Because any pattern has a price. What price, you ask? First, encapsulation is violated. Clients of the weather-service are now aware that other providers are being used _internally_. Some clients may get tempted to override providers also it\'s not under their responsibility. Second, it\'s another layer of complexity to learn, maintain, and one more way to shoot yourself in the legs. StackOverflow owes some of its revenues to Nest.js DI - plenty of discussions try to solve this puzzle (e.g. did you know that in case of circular dependencies the order of imports matters?). Third, there is the performance thing - Nest.js, for example struggled to provide a decent start time for serverless environments and had to introduce [lazy loaded modules](https://docs.nestjs.com/fundamentals/lazy-loading-modules). Don\'t get me wrong, **in some cases**, there is a good case for DI: When a need arises to decouple a dependency from its caller, or to allow clients to inject custom implementations (e.g., the strategy pattern). **In such case**, when there is a value, you may consider whether the _value of DI is worth its price_. If you don\'t have this case, why pay for nothing?\\n\\nI recommend reading the first paragraphs of this blog post [\'Dependency Injection is EVIL\'](https://www.tonymarston.net/php-mysql/dependency-injection-is-evil.html) (and absolutely don\'t agree with this bold words)\\n\\n**\u2600\ufe0f Better alternative:** \'Lean-ify\' your engineering approach - avoid using any tool unless it serves a real-world need immediately. Start simple, a dependent class should simply import its dependency and use it - Yeah, using the plain Node.js module system (\'require\'). Facing a situation when there is a need to factor dynamic objects? There are a handful of simple patterns, simpler than DI, that you should consider, like \'if/else\', factory function, and more. Are singletons requested? Consider techniques with lower costs like the module system with factory function. Need to stub/mock for testing? Monkey patching might be better than DI: better clutter your test code a bit than clutter your production code. Have a strong need to hide from an object where its dependencies are coming from? You sure? Use DI!\\n\\n```typescript\\n// humidity-service.ts - not customer facing\\nexport async function getHumidity(when: Datetime): Promise<number> {\\n  // Fetches from some specific cloud service\\n}\\n\\n// weather-service.ts - customer facing\\nimport { getHumidity } from \\"./humidity-service.ts\\";\\n\\n// \u2705 No wiring is happening externally, all is flat and explicit. Simple\\nexport async function getWeather(when: Datetime): Promise<number> {\\n  // Fetch temperature from somewhere and then humidity from GoogleHumidityService\\n  // Nobody needs to know about it, its an implementation details\\n  await getHumidity(when);\\n}\\n```\\n\\n___\\n\\n## 1 min pause: A word or two about me, the author\\n\\nMy name is Yoni Goldberg, I\'m a Node.js developer and consultant. I wrote few code-books like [JavaScript testing best practices](https://github.com/goldbergyoni/javascript-testing-best-practices) and [Node.js best practices](https://github.com/goldbergyoni/nodebestpractices) (100,000 stars \u2728\ud83e\udd79). That said, my best guide is [Node.js testing practices](https://github.com/testjavascript/nodejs-integration-tests-best-practices) which only few read \ud83d\ude1e. I shall release [an advanced Node.js testing course soon](https://testjavascript.com/) and also hold workshops for teams. I\'m also a core maintainer of [Practica.js](https://github.com/practicajs/practica) which is a Node.js starter that creates a production-ready example Node Monorepo solution that is based on the standards and simplicity. It might be your primary option when starting a new Node.js solution\\n\\n___\\n\\n## 4. Passport.js for token authentication\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** Commonly, you\'re in need to issue or/and authenticate JWT tokens. Similarly, you might need to allow login from _one_ single social network like Google/Facebook. When faced with these kinds of needs, Node.js developers rush to the glorious library [Passport.js](https://www.passportjs.org/) like butterflies are attracted to light\\n\\n**\ud83d\udcca How popular:** 1,389,720 weekly downloads\\n\\n**\ud83e\udd14 Why it might be wrong:** When tasked with guarding your routes with JWT token - you\'re just a few lines of code shy from ticking the goal. Instead of messing up with a new framework, instead of introducing levels of indirections (you call passport, then it calls you), instead of spending time learning new abstractions - use a JWT library directly. Libraries like [jsonwebtoken](https://github.com/auth0/node-jsonwebtoken) or [fast-jwt](https://github.com/nearform/fast-jwt) are simple and well maintained. Have concerns with the security hardening? Good point, your concerns are valid. But would you not get better hardening with a direct understanding of your configuration and flow? Will hiding things behind a framework help? Even if you prefer the hardening of a battle-tested framework, Passport doesn\'t handle a handful of security risks like secrets/token, secured user management, DB protection, and more. My point, you probably anyway need fully-featured user and authentication management platforms. Various cloud services and OSS projects, can tick all of those security concerns. Why then start in the first place with a framework that doesn\'t satisfy your security needs? It seems like many who opt for Passport.js are not fully aware of which needs are satisfied and which are left open. All of that said, Passport definitely shines when looking for a quick way to support _many_ social login providers\\n\\n**\u2600\ufe0f Better alternative:** Is token authentication in order? These few lines of code below might be all you need. You may also glimpse into [Practica.js wrapper around these libraries](https://github.com/practicajs/practica/tree/main/src/code-templates/libraries/jwt-token-verifier). A real-world project at scale typically need more: supporting async JWT [(JWKS)](https://auth0.com/docs/secure/tokens/json-web-tokens/json-web-key-sets), securely manage and rotate the secrets to name a few examples. In this case, OSS solution like [keycloak (https://github.com/keycloak/keycloak) or commercial options like Auth0[https://github.com/auth0] are alternatives to consider\\n\\n```javascript\\n// jwt-middleware.js, a simplified version - Refer to Practica.js to see some more corner cases\\nconst middleware = (req, res, next) => {\\n    if(!req.headers.authorization){\\n        res.sendStatus(401)\\n    }\\n\\n    jwt.verify(req.headers.authorization, options.secret, (err: any, jwtContent: any) => {\\n      if (err) {\\n        return res.sendStatus(401);\\n      }\\n\\n      req.user = jwtContent.data;\\n\\n      next();\\n    });\\n```\\n\\n## 5. Supertest for integration/API testing\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** When testing against an API (i.e., component, integration, E2E tests), the library [supertest](https://www.npmjs.com/package/supertest) provides a sweet syntax that can both detect the web server address, make HTTP call and also assert on the response. Three in one\\n\\n```javascript\\ntest(\\"When adding invalid user, then the response is 400\\", (done) => {\\n  const request = require(\\"supertest\\");\\n  const app = express();\\n  // Arrange\\n  const userToAdd = {\\n    name: undefined,\\n  };\\n\\n  // Act\\n  request(app)\\n    .post(\\"/user\\")\\n    .send(userToAdd)\\n    .expect(\\"Content-Type\\", /json/)\\n    .expect(400, done);\\n\\n  // Assert\\n  // We already asserted above \u261d\ud83c\udffb as part of the request\\n});\\n```\\n\\n**\ud83d\udcca How popular:** 2,717,744 weekly downloads\\n\\n**\ud83e\udd14 Why it might be wrong:** You already have your assertion library (Jest? Chai?), it has a great error highlighting and comparison - you trust it. Why code some tests using another assertion syntax? Not to mention, Supertest\'s assertion errors are not as descriptive as Jest and Chai. It\'s also cumbersome to mix HTTP client + assertion library instead of choosing the best for each mission. Speaking of the best, there are more standard, popular, and better-maintained HTTP clients (like fetch, axios and other friends). Need another reason? Supertest might encourage coupling the tests to Express as it offers a constructor that gets an Express object. This constructor infers the API address automatically (useful when using dynamic test ports). This couples the test to the implementation and won\'t work in the case where you wish to run the same tests against a remote process (the API doesn\'t live with the tests). My repository [\'Node.js testing best practices\'](https://github.com/testjavascript/nodejs-integration-tests-best-practices) holds examples of how tests can infer the API port and address\\n\\n**\u2600\ufe0f Better alternative:** A popular and standard HTTP client library like Node.js Fetch or Axios. In [Practica.js](https://github.com/practicajs/practica) (a Node.js starter that packs many best practices) we use Axios. It allows us to configure a HTTP client that is shared among all the tests: We bake inside a JWT token, headers, and a base URL. Another good pattern that we look at, is making each Microservice generate HTTP client library for its consumers. This brings strong-type experience to the clients, synchronizes the provider-consumer versions and as a bonus - The provider can test itself with the same library that its consumers are using\\n\\n```javascript\\ntest(\\"When adding invalid user, then the response is 400 and includes a reason\\", (done) => {\\n  const app = express();\\n  // Arrange\\n  const userToAdd = {\\n    name: undefined,\\n  };\\n\\n  // Act\\n  const receivedResponse = axios.post(\\n    `http://localhost:${apiPort}/user`,\\n    userToAdd\\n  );\\n\\n  // Assert\\n  // \u2705 Assertion happens in a dedicated stage and a dedicated library\\n  expect(receivedResponse).toMatchObject({\\n    status: 400,\\n    data: {\\n      reason: \\"no-name\\",\\n    },\\n  });\\n});\\n```\\n\\n## 6. Fastify decorate for non request/web utilities\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** [Fastify](https://github.com/fastify/fastify) introduces great patterns. Personally, I highly appreciate how it preserves the simplicity of Express while bringing more batteries. One thing that got me wondering is the \'decorate\' feature which allows placing common utilities/services inside a widely accessible container object. I\'m referring here specifically to the case where a cross-cutting concern utility/service is being used. Here is an example:\\n\\n```javascript\\n// An example of a utility that is cross-cutting-concern. Could be logger or anything else\\nfastify.decorate(\'metricsService\', function (name) {\\n  fireMetric: () => {\\n    // My code that sends metrics to the monitoring system\\n  }\\n})\\n\\nfastify.get(\'/api/orders\', async function (request, reply) {\\n  this.metricsService.fireMetric({name: \'new-request\'})\\n  // Handle the request\\n})\\n\\n// my-business-logic.js\\nexports function calculateSomething(){\\n  // How to fire a metric?\\n}\\n```\\n\\nIt should be noted that \'decoration\' is also used to place values (e.g., user) inside a request - this is a slightly different case and a sensible one\\n\\n**\ud83d\udcca How popular:** Fastify has 696,122 weekly download and growing rapidly. The decorator concept is part of the framework\'s core\\n\\n**\ud83e\udd14 Why it might be wrong:** Some services and utilities serve cross-cutting-concern needs and should be accessible from other layers like domain (i.e, business logic, DAL). When placing utilities inside this object, the Fastify object might not be accessible to these layers. You probably don\'t want to couple your web framework with your business logic: Consider that some of your business logic and repositories might get invoked from non-REST clients like CRON, MQ, and similar - In these cases, Fastify won\'t get involved at all so better not trust it to be your service locator\\n\\n**\u2600\ufe0f Better alternative:** A good old Node.js module is a standard way to expose and consume functionality. Need a singleton? Use the module system caching. Need to instantiate a service in correlation with a Fastify life-cycle hook (e.g., DB connection on start)? Call it from that Fastify hook. In the rare case where a highly dynamic and complex instantiation of dependencies is needed - DI is also a (complex) option to consider\\n\\n```javascript\\n// \u2705 A simple usage of good old Node.js modules\\n// metrics-service.js\\n\\nexports async function fireMetric(name){\\n  // My code that sends metrics to the monitoring system\\n}\\n\\nimport {fireMetric} from \'./metrics-service.js\'\\n\\nfastify.get(\'/api/orders\', async function (request, reply) {\\n  metricsService.fireMetric({name: \'new-request\'})\\n})\\n\\n// my-business-logic.js\\nexports function calculateSomething(){\\n  metricsService.fireMetric({name: \'new-request\'})\\n}\\n```\\n\\n## 7. Logging from a catch clause\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** You catch an error somewhere deep in the code (not on the route level), then call logger.error to make this error observable. Seems simple and necessary\\n\\n```javascript\\ntry{\\n    axios.post(\'https://thatService.io/api/users);\\n}\\ncatch(error){\\n    logger.error(error, this, {operation: addNewOrder});\\n}\\n```\\n\\n**\ud83d\udcca How popular:** Hard to put my hands on numbers but it\'s quite popular, right?\\n\\n**\ud83e\udd14 Why it might be wrong:** First, errors should get handled/logged in a central location. Error handling is a critical path. Various catch clauses are likely to behave differently without a centralized and unified behavior. For example, a request might arise to tag all errors with certain metadata, or on top of logging, to also fire a monitoring metric. Applying these requirements in ~100 locations is not a walk in the park. Second, catch clauses should be minimized to particular scenarios. By default, the natural flow of an error is bubbling down to the route/entry-point - from there, it will get forwarded to the error handler. Catch clauses are more verbose and error-prone - therefore it should serve two very specific needs: When one wishes to change the flow based on the error or enrich the error with more information (which is not the case in this example)\\n\\n**\u2600\ufe0f Better alternative:** By default, let the error bubble down the layers and get caught by the entry-point global catch (e.g., Express error middleware). In cases when the error should trigger a different flow (e.g., retry) or there is value in enriching the error with more context - use a catch clause. In this case, ensure the .catch code also reports to the error handler\\n\\n```javascript\\n// A case where we wish to retry upon failure\\ntry{\\n    axios.post(\'https://thatService.io/api/users);\\n}\\ncatch(error){\\n    // \u2705 A central location that handles error\\n    errorHandler.handle(error, this, {operation: addNewOrder});\\n    callTheUserService(numOfRetries++);\\n}\\n```\\n\\n## 8. Use Morgan logger for express web requests\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** In many web apps, you are likely to find a pattern that is being copy-pasted for ages - Using Morgan logger to log requests information:\\n\\n```javascript\\nconst express = require(\\"express\\");\\nconst morgan = require(\\"morgan\\");\\n\\nconst app = express();\\n\\napp.use(morgan(\\"combined\\"));\\n```\\n\\n**\ud83d\udcca How popular:** 2,901,574 downloads/week\\n\\n**\ud83e\udd14 Why it might be wrong:** Wait a second, you already have your main logger, right? Is it Pino? Winston? Something else? Great. Why deal with and configure yet another logger? I do appreciate the HTTP domain-specific language (DSL) of Morgan. The syntax is sweet! But does it justify having two loggers?\\n\\n**\u2600\ufe0f Better alternative:** Put your chosen logger in a middleware and log the desired request/response properties:\\n\\n```javascript\\n// \u2705 Use your preferred logger for all the tasks\\nconst logger = require(\\"pino\\")();\\napp.use((req, res, next) => {\\n  res.on(\\"finish\\", () => {\\n    logger.info(`${req.url} ${res.statusCode}`); // Add other properties here\\n  });\\n  next();\\n});\\n```\\n\\n## 9. Having conditional code based on `NODE_ENV` value\\n\\n**\ud83d\udc81\u200d\u2642\ufe0f What is it about:** To differentiate between development vs production configuration, it\'s common to set the environment variable NODE_ENV with \\"production|test\\". Doing so allows the various tooling to act differently. For example, some templating engines will cache compiled templates only in production. Beyond tooling, custom applications use this to specify behaviours that are unique to the development or production environment:\\n\\n```javascript\\nif (process.env.NODE_ENV === \\"production\\") {\\n  // This is unlikely to be tested since test runner usually set NODE_ENV=test\\n  setLogger({ stdout: true, prettyPrint: false });\\n  // If this code branch above exists, why not add more production-only configurations:\\n  collectMetrics();\\n} else {\\n  setLogger({ splunk: true, prettyPrint: true });\\n}\\n```\\n\\n**\ud83d\udcca How popular:** 5,034,323 code results in GitHub when searching for \\"NODE_ENV\\". It doesn\'t seem like a rare pattern\\n\\n**\ud83e\udd14 Why it might be wrong:** Anytime your code checks whether it\'s production or not, this branch won\'t get hit by default in some test runner (e.g., Jest set `NODE_ENV=test`). In _any_ test runner, the developer must remember to test for each possible value of this environment variable. In the example above, `collectMetrics()` will be tested for the first time in production. Sad smiley. Additionally, putting these conditions opens the door to add more differences between production and the developer machine - when this variable and conditions exists, a developer gets tempted to put some logic for production only. Theoretically, this can be tested: one can set `NODE_ENV = \\"production\\"` in testing and cover the production branches (if she remembers...). But then, if you can test with `NODE_ENV=\'production\'`, what\'s the point in separating? Just consider everything to be \'production\' and avoid this error-prone mental load\\n\\n**\u2600\ufe0f Better alternative:** Any code that was written by us, must be tested. This implies avoiding any form of if(production)/else(development) conditions. Wouldn\'t anyway developers machine have different surrounding infrastructure than production (e.g., logging system)? They do, the environments are quite difference, but we feel comfortable with it. These infrastructural things are battle-tested, extraneous, and not part of our code. To keep the same code between dev/prod and still use different infrastructure - we put different values in the configuration (not in the code). For example, a typical logger emits JSON in production but in a development machine it emits \'pretty-print\' colorful lines. To meet this, we set ENV VAR that tells whether what logging style we aim for:\\n\\n```javascript\\n//package.json\\n\\"scripts\\": {\\n    \\"start\\": \\"LOG_PRETTY_PRINT=false index.js\\",\\n    \\"test\\": \\"LOG_PRETTY_PRINT=true jest\\"\\n}\\n\\n//index.js\\n//\u2705 No condition, same code for all the environments. The variations are defined externally in config or deployment files\\nsetLogger({prettyPrint: process.env.LOG_PRETTY_PRINT})\\n```\\n\\n## Closing\\n\\nI hope that these thoughts, at least one of them, made you re-consider adding a new technique to your toolbox. In any case, let\'s keep our community vibrant, disruptive and kind. Respectful discussions are almost as important as the event loop. Almost.\\n\\n## Some of my other articles\\n\\n- [Book: Node.js testing best practices](https://github.com/testjavascript/nodejs-integration-tests-best-practices)\\n- [Book: JavaScript testing best practices](https://github.com/testjavascript/nodejs-integration-tests-best-practices)\\n- [How to be a better Node.js developer in 2020](https://yonigoldberg.medium.com/20-ways-to-become-a-better-node-js-developer-in-2020-d6bd73fcf424). The 2023 version is coming soon\\n- [Practica.js - A Node.js starter](https://github.com/practicajs/practica)\\n- [Node.js best practices](https://github.com/goldbergyoni/nodebestpractices)"},{"id":"practica-is-alive","metadata":{"permalink":"/blog/practica-is-alive","editUrl":"https://github.com/practicajs/practica/tree/main/docs/blog/practica-is-alive/index.md","source":"@site/blog/practica-is-alive/index.md","title":"Practica.js v0.0.1 is alive","description":"\ud83e\udd73 We\'re thrilled to launch the very first version of Practica.js.","date":"2022-07-15T10:00:00.000Z","formattedDate":"July 15, 2022","tags":[{"label":"node.js","permalink":"/blog/tags/node-js"},{"label":"express","permalink":"/blog/tags/express"},{"label":"fastify","permalink":"/blog/tags/fastify"}],"readingTime":1.21,"hasTruncateMarker":false,"authors":[{"name":"Yoni Goldberg","title":"Practica.js core maintainer","url":"https://github.com/goldbergyoni","imageURL":"https://github.com/goldbergyoni.png","key":"goldbergyoni"}],"frontMatter":{"slug":"practica-is-alive","date":"2022-07-15T10:00","hide_table_of_contents":true,"title":"Practica.js v0.0.1 is alive","authors":["goldbergyoni"],"tags":["node.js","express","fastify"]},"prevItem":{"title":"Popular Node.js patterns and tools to re-consider","permalink":"/blog/popular-nodejs-pattern-and-tools-to-reconsider"}},"content":"\ud83e\udd73 We\'re thrilled to launch the very first version of Practica.js.\\n\\n## What is Practica is one paragraph\\n\\nAlthough Node.js has great frameworks \ud83d\udc9a, they were never meant to be production ready immediately. Practica.js aims to bridge the gap. Based on your preferred framework, we generate some example code that demonstrates a full workflow, from API to DB, that is packed with good practices. For example, we include a hardened dockerfile, N-Tier folder structure, great testing templates, and more. This saves a great deal of time and can prevent painful mistakes. All decisions made are [neatly and thoughtfully documented](./decisions/index). We strive to keep things as simple and standard as possible and base our work off the popular guide: [Node.js Best Practices](https://github.com/goldbergyoni/nodebestpractices).\\n\\nYour developer experience would look as follows: Generate our starter using the CLI and get an example Node.js solution. This solution is a typical Monorepo setup with an example Microservice and libraries. All is based on super-popular libraries that we merely stitch together. It also constitutes tons of optimization - linters, libraries, Monorepo configuration, tests and much more. Inside the example Microservice you\'ll find an example flow, from API to DB. Based on this, you can modify the entity and DB fields and build you app. \\n\\n## 90 seconds video\\n\\n<iframe width=\\"1024\\" height=\\"768\\" src=\\"https://www.youtube.com/embed/F6kAs2VEcKw\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n## How to get started\\n\\nTo get up to speed quickly, read our [getting started guide](https://practica.dev/the-basics/getting-started-quickly)."}]}')}}]);